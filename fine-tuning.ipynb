{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11200209,"sourceType":"datasetVersion","datasetId":6989634}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"18ab9fe8abd84251a729fefe3bfa0ad9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23d09e6258784367bee4067b800fceb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48d7994725b24a28ac9c79418c5111a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7417c346afc24a98832d481d58167b69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae6c27aa30f34ddeba9abd06bf96cafb","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93bdf8798f5f42718bb89b101e54c682","value":2}},"7d5805c372ee40cdb860c69d0d87c87b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8e66c28e3e54a3ea111ef70b229d8a9","placeholder":"​","style":"IPY_MODEL_23d09e6258784367bee4067b800fceb9","value":"Loading checkpoint shards: 100%"}},"93bdf8798f5f42718bb89b101e54c682":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae6c27aa30f34ddeba9abd06bf96cafb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b11f7e2e0acc40128f1ffc5f36b5b029":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d5805c372ee40cdb860c69d0d87c87b","IPY_MODEL_7417c346afc24a98832d481d58167b69","IPY_MODEL_f73f2c3f4e1347dcb0ff085a27d1927a"],"layout":"IPY_MODEL_48d7994725b24a28ac9c79418c5111a6"}},"b5dbaea6b0ce4330921b1486f1c4f7d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8e66c28e3e54a3ea111ef70b229d8a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f73f2c3f4e1347dcb0ff085a27d1927a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18ab9fe8abd84251a729fefe3bfa0ad9","placeholder":"​","style":"IPY_MODEL_b5dbaea6b0ce4330921b1486f1c4f7d8","value":" 2/2 [00:34&lt;00:00, 17.13s/it]"}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting up environment","metadata":{}},{"cell_type":"markdown","source":"Check cuda version","metadata":{"id":"NdvT46W7JVp-"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"WsGKOY7CJO_p","outputId":"faf19e6b-cd94-427a-c135-534112e1cf10","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:09:54.776242Z","iopub.execute_input":"2025-04-03T07:09:54.776586Z","iopub.status.idle":"2025-04-03T07:09:55.000683Z","shell.execute_reply.started":"2025-04-03T07:09:54.776563Z","shell.execute_reply":"2025-04-03T07:09:54.999639Z"}},"outputs":[{"name":"stdout","text":"Thu Apr  3 07:09:54 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   36C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Clone GitHub repo","metadata":{}},{"cell_type":"code","source":"!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:09:55.001634Z","iopub.execute_input":"2025-04-03T07:09:55.001957Z","iopub.status.idle":"2025-04-03T07:09:56.224900Z","shell.execute_reply.started":"2025-04-03T07:09:55.001918Z","shell.execute_reply":"2025-04-03T07:09:56.223750Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'LLaMA-Factory'...\nremote: Enumerating objects: 348, done.\u001b[K\nremote: Counting objects: 100% (348/348), done.\u001b[K\nremote: Compressing objects: 100% (289/289), done.\u001b[K\nremote: Total 348 (delta 83), reused 146 (delta 44), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (348/348), 9.53 MiB | 32.95 MiB/s, done.\nResolving deltas: 100% (83/83), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Change directory","metadata":{}},{"cell_type":"code","source":"%cd LLaMA-Factory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:17:18.700076Z","iopub.execute_input":"2025-04-03T07:17:18.700453Z","iopub.status.idle":"2025-04-03T07:17:18.707191Z","shell.execute_reply.started":"2025-04-03T07:17:18.700429Z","shell.execute_reply":"2025-04-03T07:17:18.706291Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/LLaMA-Factory\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"Install packages","metadata":{"id":"U0zIj5U8IYpW"}},{"cell_type":"code","source":"!pip install -e \".[torch,metrics]\"\n!pip install deepspeed triton\n!pip install flash-attn --no-build-isolation","metadata":{"id":"qxeL2VoxJDIj","outputId":"8a8366e4-eb9a-47f3-b3ef-96e2f94e6224","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:09:56.234343Z","iopub.execute_input":"2025-04-03T07:09:56.234570Z","iopub.status.idle":"2025-04-03T07:11:14.737287Z","shell.execute_reply.started":"2025-04-03T07:09:56.234551Z","shell.execute_reply":"2025-04-03T07:11:14.736232Z"}},"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/LLaMA-Factory\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2 (from llamafactory==0.9.3.dev0)\n  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: datasets<=3.4.1,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.3.1)\nRequirement already satisfied: accelerate<=1.5.2,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.2.1)\nRequirement already satisfied: peft<=0.15.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\nCollecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.21.0)\nCollecting gradio<=5.21.0,>=4.38.0 (from llamafactory==0.9.3.dev0)\n  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.2.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.13.1)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.8.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.20.3)\nCollecting uvicorn (from llamafactory==0.9.3.dev0)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.11.0a2)\nCollecting fastapi (from llamafactory==0.9.3.dev0)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nCollecting sse-starlette (from llamafactory==0.9.3.dev0)\n  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.7.5)\nCollecting fire (from llamafactory==0.9.3.dev0)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\nRequirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\nCollecting pydantic (from llamafactory==0.9.3.dev0)\n  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\nCollecting av (from llamafactory==0.9.3.dev0)\n  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.10.2.post1)\nCollecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.2.4)\nRequirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.42.1)\nCollecting rouge-chinese (from llamafactory==0.9.3.dev0)\n  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.5.1+cu121)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.12)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.7.1)\nCollecting ffmpy (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.7.2 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\nCollecting groovy~=0.1 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.4)\nCollecting markupsafe~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.12)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.0.0)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting ruff>=0.9.3 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.12.2)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (14.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.7)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (0.7.0)\nCollecting pydantic-core==2.27.2 (from pydantic->llamafactory==0.9.3.dev0)\n  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.4.2)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2->llamafactory==0.9.3.dev0) (2024.11.6)\nRequirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\nCollecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.3.dev0) (2.5.0)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.12.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk->llamafactory==0.9.3.dev0) (1.17.0)\nCollecting anyio<5.0,>=3.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.2.2)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.18.3)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.19.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.3.dev0) (3.5.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\nDownloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\nDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\nDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\nDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nBuilding wheels for collected packages: llamafactory, fire\n  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26076 sha256=ad1f43769ac4e51d310fdff202ce3fbd17b384800366ba7e04613c7b5ae69481\n  Stored in directory: /tmp/pip-ephem-wheel-cache-2oai8x62/wheels/21/5a/a2/9a8fea19e68e32089e22401d08554f51119f2464cad3a126ec\n  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=c97e3cbaee75cdc7702a8f37bd316b33456eaf41a0a75738dc454c77b0ee1ff4\n  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\nSuccessfully built llamafactory fire\nInstalling collected packages: uvicorn, tomlkit, shtab, semantic-version, ruff, rouge-chinese, python-multipart, pydantic-core, markupsafe, groovy, fire, ffmpy, av, anyio, starlette, pydantic, tyro, sse-starlette, safehttpx, gradio-client, fastapi, transformers, trl, gradio, llamafactory\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.29.0\n    Uninstalling pydantic_core-2.29.0:\n      Successfully uninstalled pydantic_core-2.29.0\n  Attempting uninstall: markupsafe\n    Found existing installation: MarkupSafe 3.0.2\n    Uninstalling MarkupSafe-3.0.2:\n      Successfully uninstalled MarkupSafe-3.0.2\n  Attempting uninstall: anyio\n    Found existing installation: anyio 3.7.1\n    Uninstalling anyio-3.7.1:\n      Successfully uninstalled anyio-3.7.1\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.11.0a2\n    Uninstalling pydantic-2.11.0a2:\n      Successfully uninstalled pydantic-2.11.0a2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed anyio-4.9.0 av-14.2.0 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 llamafactory-0.9.3.dev0 markupsafe-2.1.5 pydantic-2.10.6 pydantic-core-2.27.2 python-multipart-0.0.20 rouge-chinese-1.0.3 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.2.1 starlette-0.46.1 tomlkit-0.13.2 transformers-4.50.0 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.0\nCollecting deepspeed\n  Downloading deepspeed-0.16.5.tar.gz (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting triton\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed) (0.8.0)\nCollecting hjson (from deepspeed)\n  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.1.0)\nRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\nRequirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.10.6)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.5.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.67.1)\nRequirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from deepspeed) (12.570.86)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.5)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deepspeed) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deepspeed) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->deepspeed) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->deepspeed) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->deepspeed) (2024.2.0)\nDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: deepspeed\n  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for deepspeed: filename=deepspeed-0.16.5-py3-none-any.whl size=1580580 sha256=0f1ef993ca6ebfd88ae63e6a33dc3acc739616d2574d30c1ae8622174ad8aefe\n  Stored in directory: /root/.cache/pip/wheels/cb/fa/e7/98efc76db11fac734a4fae8c19dd08cc24257107e132e674f6\nSuccessfully built deepspeed\nInstalling collected packages: triton, hjson, deepspeed\nSuccessfully installed deepspeed-0.16.5 hjson-3.1.0 triton-3.2.0\nCollecting flash-attn\n  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1+cu121)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\nBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187797312 sha256=b267f80a08e516292cdd748056a2178a45b8abedf7fca123292eb17c21c8c87c\n  Stored in directory: /root/.cache/pip/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca\nSuccessfully built flash-attn\nInstalling collected packages: flash-attn\nSuccessfully installed flash-attn-2.7.4.post1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"markdown","source":"Import packages","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets, Value, Dataset\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:16:14.050749Z","iopub.execute_input":"2025-04-03T07:16:14.051032Z","iopub.status.idle":"2025-04-03T07:16:14.055070Z","shell.execute_reply.started":"2025-04-03T07:16:14.051011Z","shell.execute_reply":"2025-04-03T07:16:14.054196Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"Load videos mapping","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/d/seanjeanmoey/next-qa-dataset/map_vid_vidorID.json\") as file:\n    video_dir_map = json.load(file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:11:16.513396Z","iopub.execute_input":"2025-04-03T07:11:16.513748Z","iopub.status.idle":"2025-04-03T07:11:16.535426Z","shell.execute_reply.started":"2025-04-03T07:11:16.513725Z","shell.execute_reply":"2025-04-03T07:11:16.534844Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"Format data","metadata":{}},{"cell_type":"code","source":"def format_data(sample):\n    return {\n        \"messages\": [\n            {\n                \"content\": f\"<video>{sample['question']}\",\n                \"role\": \"user\"\n            },\n            {\n                \"content\": f\"{sample['answer']}\",\n                \"role\": \"assistant\"\n            }\n        ],\n        \"videos\": [\n            f\"/kaggle/input/d/seanjeanmoey/next-qa-dataset/NExTVideo/NExTVideo/{video_dir_map[sample['video']]}.mp4\"\n        ]\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:27:31.295373Z","iopub.execute_input":"2025-04-03T07:27:31.295770Z","iopub.status.idle":"2025-04-03T07:27:31.301368Z","shell.execute_reply.started":"2025-04-03T07:27:31.295729Z","shell.execute_reply":"2025-04-03T07:27:31.300582Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"Format MCQ","metadata":{}},{"cell_type":"code","source":"def reformat_mcq(sample):\n    choice_labels = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n    choices = [sample[f\"a{i}\"] for i in range(5)]\n    formatted_choices = \"\\n\".join([f\"{choice_labels[i]}. {choice}\" for i, choice in enumerate(choices)])\n    \n    return {\n        \"video\": sample[\"video\"],\n        \"frame_count\": sample[\"frame_count\"],\n        \"width\": sample[\"width\"],\n        \"height\": sample[\"height\"],\n        \"question\": f\"{sample['question']}\\n{formatted_choices}\\nSelect one best answer to the above multiple-choice question based on the video. Respond with only the letter (A, B, C, D or E) of the correct option.\",\n        \"answer\": choice_labels[sample[\"answer\"]],\n        \"qid\": sample[\"qid\"],\n        \"type\": sample[\"type\"],\n        \"additional_ref_answer\": None\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:11:16.540973Z","iopub.execute_input":"2025-04-03T07:11:16.541195Z","iopub.status.idle":"2025-04-03T07:11:16.555722Z","shell.execute_reply.started":"2025-04-03T07:11:16.541176Z","shell.execute_reply":"2025-04-03T07:11:16.554986Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Load dataset","metadata":{}},{"cell_type":"code","source":"dataset_id = 'lmms-lab/NExTQA'\n\nmcq_dataset = load_dataset(dataset_id, 'MC')['test'].map(reformat_mcq, remove_columns=['a0', 'a1', 'a2', 'a3', 'a4'])\nnew_features = mcq_dataset.features.copy()\nnew_features[\"video\"] = Value(\"string\")\nnew_features[\"frame_count\"] = Value(\"int32\")\nnew_features[\"width\"] = Value(\"int32\")\nnew_features[\"height\"] = Value(\"int32\")\nnew_features[\"qid\"] = Value(\"int32\")\nmcq_dataset = mcq_dataset.cast(new_features)\ntrain_test_split = mcq_dataset.train_test_split(test_size=0.3, seed=42)\nval_test_split = train_test_split['test'].train_test_split(test_size=2/3, seed=42)\nmcq_train_dataset = train_test_split['train']\nmcq_eval_dataset = val_test_split['train']\nmcq_test_dataset = val_test_split['test']\n\noe_train_dataset, oe_eval_dataset, oe_test_dataset = load_dataset(dataset_id, 'OE', split=['train', 'validation', 'test'])\n\ntrain_dataset = concatenate_datasets([mcq_train_dataset, oe_train_dataset])\neval_dataset = concatenate_datasets([mcq_eval_dataset, oe_eval_dataset])\ntest_dataset = concatenate_datasets([mcq_test_dataset, oe_test_dataset])\n\ntrain_dataset = [format_data(sample) for sample in train_dataset]\neval_dataset = [format_data(sample) for sample in eval_dataset]\ntest_dataset = [format_data(sample) for sample in test_dataset]\n\ndataset = train_dataset + eval_dataset + test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:27:35.340005Z","iopub.execute_input":"2025-04-03T07:27:35.340367Z","iopub.status.idle":"2025-04-03T07:27:45.566441Z","shell.execute_reply.started":"2025-04-03T07:27:35.340337Z","shell.execute_reply":"2025-04-03T07:27:45.565685Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"Create dataset info json","metadata":{}},{"cell_type":"code","source":"args = { \n    \"nextqa\": {\n        \"file_name\": \"/kaggle/working/LLaMA-Factory/data/nextqa.json\",\n        \"formatting\": \"sharegpt\",\n        \"columns\": {\n            \"messages\": \"messages\",\n            \"videos\": \"videos\"\n        },\n        \"tags\": {\n            \"role_tag\": \"role\",\n            \"content_tag\": \"content\",\n            \"user_tag\": \"user\",\n            \"assistant_tag\": \"assistant\"\n        }\n    }\n}\nwith open(\"data/dataset_info.json\", \"w\", encoding=\"utf-8\") as f: \n    json.dump(args, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:27:48.683777Z","iopub.execute_input":"2025-04-03T07:27:48.684065Z","iopub.status.idle":"2025-04-03T07:27:48.688972Z","shell.execute_reply.started":"2025-04-03T07:27:48.684044Z","shell.execute_reply":"2025-04-03T07:27:48.688044Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"Create dataset json","metadata":{}},{"cell_type":"code","source":"with open(\"data/nextqa.json\", \"w\", encoding=\"utf-8\") as f: \n    json.dump(dataset, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:27:50.826576Z","iopub.execute_input":"2025-04-03T07:27:50.826923Z","iopub.status.idle":"2025-04-03T07:27:51.994148Z","shell.execute_reply.started":"2025-04-03T07:27:50.826895Z","shell.execute_reply":"2025-04-03T07:27:51.993031Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"# Fine-tuning Model","metadata":{}},{"cell_type":"markdown","source":"Import packages","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:11:31.790281Z","iopub.execute_input":"2025-04-03T07:11:31.790486Z","iopub.status.idle":"2025-04-03T07:11:33.532366Z","shell.execute_reply.started":"2025-04-03T07:11:31.790467Z","shell.execute_reply":"2025-04-03T07:11:33.531704Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Login to wandb","metadata":{}},{"cell_type":"code","source":"wandb.login(key=UserSecretsClient().get_secret(\"WANDB_API_KEY\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:11:33.535183Z","iopub.execute_input":"2025-04-03T07:11:33.535401Z","iopub.status.idle":"2025-04-03T07:11:39.978548Z","shell.execute_reply.started":"2025-04-03T07:11:33.535375Z","shell.execute_reply":"2025-04-03T07:11:39.977815Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseanjeanmoey123\u001b[0m (\u001b[33mseanjeanmoey123-nus\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"Create train.json","metadata":{}},{"cell_type":"code","source":"args = {\n    \"model_name_or_path\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n    \"image_max_pixels\": 262144,\n    \"video_max_pixels\": 16384,\n    \"trust_remote_code\": True,\n    \"stage\": \"sft\",\n    \"do_train\": True,\n    \"finetuning_type\": \"lora\",\n    \"flash_attn\": \"auto\",\n    \"lora_rank\": 8,\n    \"lora_alpha\": 16,\n    \"lora_dropout\": 0,\n    \"lora_target\": \"all\",\n    \"dataset_dir\": \"/kaggle/working/LLaMA-Factory/data\",\n    \"dataset\": \"nextqa\",\n    \"template\": \"qwen2_vl\",\n    \"overwrite_cache\": True,\n    # \"cutoff_len\": 2048,\n    # \"max_samples\": 128,\n    # \"preprocessing_num_workers\": 16,\n    \"dataloader_num_workers\": 4,\n    \"output_dir\": \"/kaggle/working/finetuned\",\n    \"logging_steps\": 5,\n    \"save_steps\": 100,\n    \"plot_loss\": True,\n    \"overwrite_output_dir\": True,\n    \"save_only_model\": False,\n    \"per_device_train_batch_size\": 1,\n    \"gradient_accumulation_steps\": 8,\n    \"learning_rate\": 5.0e-5,\n    \"num_train_epochs\": 3.0,\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.1,\n    \"bf16\": True,\n    \"ddp_timeout\": 180000000,\n    \"resume_from_checkpoint\": None,\n    \"max_grad_norm\": 1,\n    \"warmup_steps\": 0,\n    \"packing\": False,\n    \"report_to\": None,\n    \"optim\": \"adamw_torch\",\n    \"streaming\": True,\n    \"max_steps\": 10000,\n}\nwith open(\"train.json\", \"w\", encoding=\"utf-8\") as f: \n    json.dump(args, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:27:55.271613Z","iopub.execute_input":"2025-04-03T07:27:55.271930Z","iopub.status.idle":"2025-04-03T07:27:55.278452Z","shell.execute_reply.started":"2025-04-03T07:27:55.271903Z","shell.execute_reply":"2025-04-03T07:27:55.277539Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"Train model","metadata":{}},{"cell_type":"code","source":"!llamafactory-cli train train.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:27:58.427249Z","iopub.execute_input":"2025-04-03T07:27:58.427582Z"}},"outputs":[{"name":"stdout","text":"2025-04-03 07:28:03.795557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-03 07:28:03.819062: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-03 07:28:03.825818: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[2025-04-03 07:28:07,294] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[INFO|2025-04-03 07:28:11] llamafactory.cli:143 >> Initializing 2 distributed tasks at: 127.0.0.1:54861\nW0403 07:28:13.539000 1500 torch/distributed/run.py:793] \nW0403 07:28:13.539000 1500 torch/distributed/run.py:793] *****************************************\nW0403 07:28:13.539000 1500 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0403 07:28:13.539000 1500 torch/distributed/run.py:793] *****************************************\n2025-04-03 07:28:19.025114: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-03 07:28:19.025112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-03 07:28:19.047298: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-03 07:28:19.047757: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-03 07:28:19.054027: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-04-03 07:28:19.054636: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[2025-04-03 07:28:22,917] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2025-04-03 07:28:22,941] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[WARNING|2025-04-03 07:28:25] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n[INFO|2025-04-03 07:28:25] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n[INFO|2025-04-03 07:28:25] llamafactory.hparams.parser:383 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:25,536 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:25,537 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:25,537 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:25,537 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:25,537 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:25,537 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:25,537 >> loading file chat_template.jinja from cache at None\n[INFO|tokenization_utils_base.py:2323] 2025-04-03 07:28:25,924 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[INFO|image_processing_base.py:381] 2025-04-03 07:28:26,288 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n[INFO|image_processing_base.py:381] 2025-04-03 07:28:26,410 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n[WARNING|logging.py:329] 2025-04-03 07:28:26,410 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n[INFO|image_processing_base.py:434] 2025-04-03 07:28:26,411 >> Image processor Qwen2VLImageProcessor {\n  \"do_convert_rgb\": true,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.48145466,\n    0.4578275,\n    0.40821073\n  ],\n  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n  \"image_std\": [\n    0.26862954,\n    0.26130258,\n    0.27577711\n  ],\n  \"max_pixels\": 12845056,\n  \"merge_size\": 2,\n  \"min_pixels\": 3136,\n  \"patch_size\": 14,\n  \"processor_class\": \"Qwen2_5_VLProcessor\",\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"longest_edge\": 12845056,\n    \"shortest_edge\": 3136\n  },\n  \"temporal_patch_size\": 2\n}\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:26,535 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:26,535 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:26,535 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:26,535 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:26,535 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:26,535 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-03 07:28:26,535 >> loading file chat_template.jinja from cache at None\n[INFO|tokenization_utils_base.py:2323] 2025-04-03 07:28:26,908 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[INFO|processing_utils.py:876] 2025-04-03 07:28:28,018 >> Processor Qwen2_5_VLProcessor:\n- image_processor: Qwen2VLImageProcessor {\n  \"do_convert_rgb\": true,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.48145466,\n    0.4578275,\n    0.40821073\n  ],\n  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n  \"image_std\": [\n    0.26862954,\n    0.26130258,\n    0.27577711\n  ],\n  \"max_pixels\": 12845056,\n  \"merge_size\": 2,\n  \"min_pixels\": 3136,\n  \"patch_size\": 14,\n  \"processor_class\": \"Qwen2_5_VLProcessor\",\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"longest_edge\": 12845056,\n    \"shortest_edge\": 3136\n  },\n  \"temporal_patch_size\": 2\n}\n\n- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n}\n)\n\n{\n  \"processor_class\": \"Qwen2_5_VLProcessor\"\n}\n\n[rank1]:[W403 07:28:28.524586139 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n[INFO|2025-04-03 07:28:28] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n[INFO|2025-04-03 07:28:28] llamafactory.data.loader:143 >> Loading dataset /kaggle/working/LLaMA-Factory/data/nextqa.json...\nGenerating train split: 60608 examples [00:00, 61411.49 examples/s]\n[rank0]:[W403 07:28:29.752984781 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\ntraining example:\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Merging Fine-tuned Model","metadata":{}},{"cell_type":"markdown","source":"Import packages","metadata":{}},{"cell_type":"code","source":"import json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:13:25.726460Z","iopub.execute_input":"2025-04-03T07:13:25.726813Z","iopub.status.idle":"2025-04-03T07:13:25.730677Z","shell.execute_reply.started":"2025-04-03T07:13:25.726778Z","shell.execute_reply":"2025-04-03T07:13:25.729811Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"Create merging script","metadata":{}},{"cell_type":"code","source":"args = {\n    \"model_name_or_path\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n    \"adapter_name_or_path\": \"/kaggle/working/finetuned\",\n    \"template\": \"qwen2_vl\",\n    \"finetuning_type\": \"lora\",\n    \"trust_remote_code\": True,\n    \"export_dir\": \"/kaggle/working/merged\",\n    \"export_size\": 5,\n    \"export_device\": \"cpu\",\n    \"export_legacy_format\": False,\n}\nwith open(\"merge.json\", \"w\", encoding=\"utf-8\") as f: \n    json.dump(args, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:13:25.731607Z","iopub.execute_input":"2025-04-03T07:13:25.731875Z","iopub.status.idle":"2025-04-03T07:13:25.748203Z","shell.execute_reply.started":"2025-04-03T07:13:25.731845Z","shell.execute_reply":"2025-04-03T07:13:25.747397Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"Merge model","metadata":{}},{"cell_type":"code","source":"!llamafactory-cli export merge.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:13:25.749047Z","iopub.execute_input":"2025-04-03T07:13:25.749357Z","iopub.status.idle":"2025-04-03T07:13:30.727975Z","shell.execute_reply.started":"2025-04-03T07:13:25.749329Z","shell.execute_reply":"2025-04-03T07:13:30.726899Z"}},"outputs":[{"name":"stdout","text":"^C\nTraceback (most recent call last):\n  File \"/usr/local/bin/llamafactory-cli\", line 5, in <module>\n    from llamafactory.cli import main\n  File \"/kaggle/working/LLaMA-Factory/src/llamafactory/__init__.py\", line 43, in <module>\n    from .extras.env import VERSION\n  File \"/kaggle/working/LLaMA-Factory/src/llamafactory/extras/env.py\", line 22, in <module>\n    import peft\n  File \"/usr/local/lib/python3.10/dist-packages/peft/__init__.py\", line 22, in <module>\n    from .auto import (\n  File \"/usr/local/lib/python3.10/dist-packages/peft/auto.py\", line 21, in <module>\n    from transformers import (\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1957, in __getattr__\n    value = getattr(module, name)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1956, in __getattr__\n    module = self._get_module(self._class_to_module[name])\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1968, in _get_module\n    return importlib.import_module(\".\" + module_name, self.__name__)\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py\", line 22, in <module>\n    from .auto_factory import (\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 40, in <module>\n    from ...generation import GenerationMixin\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1956, in __getattr__\n    module = self._get_module(self._class_to_module[name])\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1968, in _get_module\n    return importlib.import_module(\".\" + module_name, self.__name__)\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 30, in <module>\n    from transformers.generation.candidate_generator import AssistantVocabTranslatorCache\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/candidate_generator.py\", line 29, in <module>\n    from ..cache_utils import DynamicCache\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py\", line 11, in <module>\n    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_6\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\", line 50, in <module>\n    from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/parallel/__init__.py\", line 2, in <module>\n    from torch.distributed.tensor.parallel.api import parallelize_module\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/parallel/api.py\", line 13, in <module>\n    from torch.distributed.tensor.parallel._utils import _validate_tp_mesh_dim\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/parallel/_utils.py\", line 11, in <module>\n    from torch._dynamo.external_utils import is_compiling as is_torchdynamo_compiling\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\", line 3, in <module>\n    from . import convert_frame, eval_frame, resume_execution\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 53, in <module>\n    from . import config, exc, trace_rules\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/trace_rules.py\", line 46, in <module>\n    from .variables import (\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/__init__.py\", line 1, in <module>\n    from .base import VariableTracker\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/base.py\", line 10, in <module>\n    from ..source import AttrSource, Source\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/source.py\", line 127, in <module>\n    class RandomValueSource(Source):\n  File \"/usr/lib/python3.10/dataclasses.py\", line 1175, in wrap\n    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n  File \"/usr/lib/python3.10/dataclasses.py\", line 1053, in _process_class\n    _cmp_fn('__eq__', '==',\n  File \"/usr/lib/python3.10/dataclasses.py\", line 629, in _cmp_fn\n    return _create_fn(name,\n  File \"/usr/lib/python3.10/dataclasses.py\", line 432, in _create_fn\n    exec(txt, globals, ns)\n  File \"<string>\", line 1, in <module>\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"Zip the output","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!7z a -r finetuned.zip finetuned\n!7z a -r merged.zip merged","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:13:30.729089Z","iopub.execute_input":"2025-04-03T07:13:30.729440Z","iopub.status.idle":"2025-04-03T07:13:31.013132Z","shell.execute_reply.started":"2025-04-03T07:13:30.729401Z","shell.execute_reply":"2025-04-03T07:13:31.012237Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs Intel(R) Xeon(R) CPU @ 2.00GHz (50653),ASM,AES-NI)\n\nScanning the drive:\n  0M Sca        0 files, 0 bytes\n\nCreating archive: finetuned.zip\n\nItems to compress: 0\n\n    \nFiles read from disk: 0\nArchive size: 22 bytes (1 KiB)\nEverything is Ok\n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs Intel(R) Xeon(R) CPU @ 2.00GHz (50653),ASM,AES-NI)\n\nScanning the drive:\n  0M Sca        0 files, 0 bytes\n\nCreating archive: merged.zip\n\nItems to compress: 0\n\n    \nFiles read from disk: 0\nArchive size: 22 bytes (1 KiB)\nEverything is Ok\n","output_type":"stream"}],"execution_count":20}]}