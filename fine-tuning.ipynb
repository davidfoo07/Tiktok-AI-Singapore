{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11200209,"sourceType":"datasetVersion","datasetId":6989634}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"18ab9fe8abd84251a729fefe3bfa0ad9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23d09e6258784367bee4067b800fceb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48d7994725b24a28ac9c79418c5111a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7417c346afc24a98832d481d58167b69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae6c27aa30f34ddeba9abd06bf96cafb","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93bdf8798f5f42718bb89b101e54c682","value":2}},"7d5805c372ee40cdb860c69d0d87c87b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8e66c28e3e54a3ea111ef70b229d8a9","placeholder":"​","style":"IPY_MODEL_23d09e6258784367bee4067b800fceb9","value":"Loading checkpoint shards: 100%"}},"93bdf8798f5f42718bb89b101e54c682":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae6c27aa30f34ddeba9abd06bf96cafb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b11f7e2e0acc40128f1ffc5f36b5b029":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d5805c372ee40cdb860c69d0d87c87b","IPY_MODEL_7417c346afc24a98832d481d58167b69","IPY_MODEL_f73f2c3f4e1347dcb0ff085a27d1927a"],"layout":"IPY_MODEL_48d7994725b24a28ac9c79418c5111a6"}},"b5dbaea6b0ce4330921b1486f1c4f7d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8e66c28e3e54a3ea111ef70b229d8a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f73f2c3f4e1347dcb0ff085a27d1927a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18ab9fe8abd84251a729fefe3bfa0ad9","placeholder":"​","style":"IPY_MODEL_b5dbaea6b0ce4330921b1486f1c4f7d8","value":" 2/2 [00:34&lt;00:00, 17.13s/it]"}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting up environment","metadata":{}},{"cell_type":"markdown","source":"Check cuda version","metadata":{"id":"NdvT46W7JVp-"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"WsGKOY7CJO_p","outputId":"faf19e6b-cd94-427a-c135-534112e1cf10","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:54:04.433012Z","iopub.execute_input":"2025-04-01T08:54:04.433342Z","iopub.status.idle":"2025-04-01T08:54:04.652202Z","shell.execute_reply.started":"2025-04-01T08:54:04.433319Z","shell.execute_reply":"2025-04-01T08:54:04.651377Z"}},"outputs":[{"name":"stdout","text":"Tue Apr  1 08:54:04 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   46C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   49C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Clone GitHub repo","metadata":{}},{"cell_type":"code","source":"!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:25:12.996561Z","iopub.execute_input":"2025-04-01T16:25:12.996911Z","iopub.status.idle":"2025-04-01T16:25:14.098596Z","shell.execute_reply.started":"2025-04-01T16:25:12.996881Z","shell.execute_reply":"2025-04-01T16:25:14.097576Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'LLaMA-Factory'...\nremote: Enumerating objects: 348, done.\u001b[K\nremote: Counting objects: 100% (348/348), done.\u001b[K\nremote: Compressing objects: 100% (289/289), done.\u001b[K\nremote: Total 348 (delta 82), reused 163 (delta 44), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (348/348), 9.53 MiB | 32.86 MiB/s, done.\nResolving deltas: 100% (82/82), done.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Change directory","metadata":{}},{"cell_type":"code","source":"%cd LLaMA-Factory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:25:16.717656Z","iopub.execute_input":"2025-04-01T16:25:16.718031Z","iopub.status.idle":"2025-04-01T16:25:16.724631Z","shell.execute_reply.started":"2025-04-01T16:25:16.718000Z","shell.execute_reply":"2025-04-01T16:25:16.723844Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/LLaMA-Factory\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Install packages","metadata":{"id":"U0zIj5U8IYpW"}},{"cell_type":"code","source":"!pip install -e \".[torch,metrics]\"\n!pip install deepspeed triton\n!pip install flash-attn --no-build-isolation","metadata":{"id":"qxeL2VoxJDIj","outputId":"8a8366e4-eb9a-47f3-b3ef-96e2f94e6224","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:25:18.482816Z","iopub.execute_input":"2025-04-01T16:25:18.483146Z","iopub.status.idle":"2025-04-01T16:26:37.572200Z","shell.execute_reply.started":"2025-04-01T16:25:18.483122Z","shell.execute_reply":"2025-04-01T16:26:37.571075Z"}},"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/LLaMA-Factory\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2 (from llamafactory==0.9.3.dev0)\n  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: datasets<=3.4.1,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.3.1)\nRequirement already satisfied: accelerate<=1.5.2,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.2.1)\nRequirement already satisfied: peft<=0.15.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\nCollecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.21.0)\nCollecting gradio<=5.21.0,>=4.38.0 (from llamafactory==0.9.3.dev0)\n  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.2.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.13.1)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.8.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.20.3)\nCollecting uvicorn (from llamafactory==0.9.3.dev0)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.11.0a2)\nCollecting fastapi (from llamafactory==0.9.3.dev0)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nCollecting sse-starlette (from llamafactory==0.9.3.dev0)\n  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.7.5)\nCollecting fire (from llamafactory==0.9.3.dev0)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\nRequirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\nCollecting pydantic (from llamafactory==0.9.3.dev0)\n  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\nCollecting av (from llamafactory==0.9.3.dev0)\n  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.10.2.post1)\nCollecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.5.1+cu121)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.2.4)\nRequirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.42.1)\nCollecting rouge-chinese (from llamafactory==0.9.3.dev0)\n  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.12)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.7.1)\nCollecting ffmpy (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.7.2 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\nCollecting groovy~=0.1 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.4)\nCollecting markupsafe~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.12)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.0.0)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting ruff>=0.9.3 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.12.2)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (14.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.7)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (0.7.0)\nCollecting pydantic-core==2.27.2 (from pydantic->llamafactory==0.9.3.dev0)\n  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.4.2)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2->llamafactory==0.9.3.dev0) (2024.11.6)\nRequirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\nCollecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.3.dev0) (2.5.0)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.12.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk->llamafactory==0.9.3.dev0) (1.17.0)\nCollecting anyio<5.0,>=3.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.2.2)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.18.3)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.19.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.3.dev0) (3.5.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\nDownloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\nDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\nDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\nDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nBuilding wheels for collected packages: llamafactory, fire\n  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26076 sha256=d6e6cbacbef589709f1dbf4ea6cd31a3a3d2165a08827aee7935300e29820123\n  Stored in directory: /tmp/pip-ephem-wheel-cache-hmge1m41/wheels/21/5a/a2/9a8fea19e68e32089e22401d08554f51119f2464cad3a126ec\n  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=8225b1344ff1b367325a9f8a1fce108d916edc05a180d84be46537e130d13060\n  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\nSuccessfully built llamafactory fire\nInstalling collected packages: uvicorn, tomlkit, shtab, semantic-version, ruff, rouge-chinese, python-multipart, pydantic-core, markupsafe, groovy, fire, ffmpy, av, anyio, starlette, pydantic, tyro, sse-starlette, safehttpx, gradio-client, fastapi, transformers, trl, gradio, llamafactory\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.29.0\n    Uninstalling pydantic_core-2.29.0:\n      Successfully uninstalled pydantic_core-2.29.0\n  Attempting uninstall: markupsafe\n    Found existing installation: MarkupSafe 3.0.2\n    Uninstalling MarkupSafe-3.0.2:\n      Successfully uninstalled MarkupSafe-3.0.2\n  Attempting uninstall: anyio\n    Found existing installation: anyio 3.7.1\n    Uninstalling anyio-3.7.1:\n      Successfully uninstalled anyio-3.7.1\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.11.0a2\n    Uninstalling pydantic-2.11.0a2:\n      Successfully uninstalled pydantic-2.11.0a2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed anyio-4.9.0 av-14.2.0 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 llamafactory-0.9.3.dev0 markupsafe-2.1.5 pydantic-2.10.6 pydantic-core-2.27.2 python-multipart-0.0.20 rouge-chinese-1.0.3 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.2.1 starlette-0.46.1 tomlkit-0.13.2 transformers-4.50.0 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.0\nCollecting deepspeed\n  Downloading deepspeed-0.16.5.tar.gz (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting triton\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed) (0.8.0)\nCollecting hjson (from deepspeed)\n  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.1.0)\nRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\nRequirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.10.6)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.5.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.67.1)\nRequirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from deepspeed) (12.570.86)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.5)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deepspeed) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deepspeed) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->deepspeed) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->deepspeed) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->deepspeed) (2024.2.0)\nDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: deepspeed\n  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for deepspeed: filename=deepspeed-0.16.5-py3-none-any.whl size=1580585 sha256=46cc0c3c294deffd0762ef198ec8a1465d4ade5ea428a29b139a8112f503bbb7\n  Stored in directory: /root/.cache/pip/wheels/cb/fa/e7/98efc76db11fac734a4fae8c19dd08cc24257107e132e674f6\nSuccessfully built deepspeed\nInstalling collected packages: triton, hjson, deepspeed\nSuccessfully installed deepspeed-0.16.5 hjson-3.1.0 triton-3.2.0\nCollecting flash-attn\n  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1+cu121)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\nBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187797312 sha256=b267f80a08e516292cdd748056a2178a45b8abedf7fca123292eb17c21c8c87c\n  Stored in directory: /root/.cache/pip/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca\nSuccessfully built flash-attn\nInstalling collected packages: flash-attn\nSuccessfully installed flash-attn-2.7.4.post1\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"markdown","source":"Import packages","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets, Value\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:26:52.831987Z","iopub.execute_input":"2025-04-01T16:26:52.832307Z","iopub.status.idle":"2025-04-01T16:26:54.485129Z","shell.execute_reply.started":"2025-04-01T16:26:52.832277Z","shell.execute_reply":"2025-04-01T16:26:54.484245Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Load videos mapping","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/d/seanjeanmoey/next-qa-dataset/map_vid_vidorID.json\") as file:\n    video_dir_map = json.load(file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:26:56.781328Z","iopub.execute_input":"2025-04-01T16:26:56.781840Z","iopub.status.idle":"2025-04-01T16:26:56.796859Z","shell.execute_reply.started":"2025-04-01T16:26:56.781809Z","shell.execute_reply":"2025-04-01T16:26:56.796064Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Format data","metadata":{}},{"cell_type":"code","source":"def format_data(sample):\n    return {\n        \"messages\": [\n            {\n                \"content\": f\"<video>{sample['question']}\",\n                \"role\": \"user\"\n            },\n            {\n                \"content\": f\"{sample['answer']}\",\n                \"role\": \"assistant\"\n            }\n        ],\n        \"videos\": [\n            f\"/kaggle/input/d/seanjeanmoey/next-qa-dataset/NExTVideo/NExTVideo/{video_dir_map[sample['video']]}.mp4\"\n        ]\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:27:00.181708Z","iopub.execute_input":"2025-04-01T16:27:00.182151Z","iopub.status.idle":"2025-04-01T16:27:00.186436Z","shell.execute_reply.started":"2025-04-01T16:27:00.182114Z","shell.execute_reply":"2025-04-01T16:27:00.185465Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Format MCQ","metadata":{}},{"cell_type":"code","source":"def reformat_mcq(sample):\n    choice_labels = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n    choices = [sample[f\"a{i}\"] for i in range(5)]\n    formatted_choices = \"\\n\".join([f\"{choice_labels[i]}. {choice}\" for i, choice in enumerate(choices)])\n    \n    return {\n        \"video\": sample[\"video\"],\n        \"frame_count\": sample[\"frame_count\"],\n        \"width\": sample[\"width\"],\n        \"height\": sample[\"height\"],\n        \"question\": f\"{sample['question']}\\n{formatted_choices}\\nSelect one best answer to the above multiple-choice question based on the video. Respond with only the letter (A, B, C, D or E) of the correct option.\",\n        \"answer\": choice_labels[sample[\"answer\"]],\n        \"qid\": sample[\"qid\"],\n        \"type\": sample[\"type\"],\n        \"additional_ref_answer\": None\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:27:02.142320Z","iopub.execute_input":"2025-04-01T16:27:02.142625Z","iopub.status.idle":"2025-04-01T16:27:02.147531Z","shell.execute_reply.started":"2025-04-01T16:27:02.142598Z","shell.execute_reply":"2025-04-01T16:27:02.146673Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Load dataset","metadata":{}},{"cell_type":"code","source":"dataset_id = 'lmms-lab/NExTQA'\n\nmcq_dataset = load_dataset(dataset_id, 'MC')['test'].map(reformat_mcq, remove_columns=['a0', 'a1', 'a2', 'a3', 'a4'])\nnew_features = mcq_dataset.features.copy()\nnew_features[\"video\"] = Value(\"string\")\nnew_features[\"frame_count\"] = Value(\"int32\")\nnew_features[\"width\"] = Value(\"int32\")\nnew_features[\"height\"] = Value(\"int32\")\nnew_features[\"qid\"] = Value(\"int32\")\nmcq_dataset = mcq_dataset.cast(new_features)\ntrain_test_split = mcq_dataset.train_test_split(test_size=0.3, seed=42)\nval_test_split = train_test_split['test'].train_test_split(test_size=2/3, seed=42)\nmcq_train_dataset = train_test_split['train']\nmcq_eval_dataset = val_test_split['train']\nmcq_test_dataset = val_test_split['test']\n\noe_train_dataset, oe_eval_dataset, oe_test_dataset = load_dataset(dataset_id, 'OE', split=['train', 'validation', 'test'])\n\ntrain_dataset = concatenate_datasets([mcq_train_dataset, oe_train_dataset])\neval_dataset = concatenate_datasets([mcq_eval_dataset, oe_eval_dataset])\ntest_dataset = concatenate_datasets([mcq_test_dataset, oe_test_dataset])\n\ntrain_dataset = [format_data(sample) for sample in train_dataset]\neval_dataset = [format_data(sample) for sample in eval_dataset]\ntest_dataset = [format_data(sample) for sample in test_dataset]\n\ndataset = train_dataset + eval_dataset + test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:27:04.035065Z","iopub.execute_input":"2025-04-01T16:27:04.035454Z","iopub.status.idle":"2025-04-01T16:27:16.269496Z","shell.execute_reply.started":"2025-04-01T16:27:04.035423Z","shell.execute_reply":"2025-04-01T16:27:16.268844Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"473ec56b386d47b1bbb51a50146e1abe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f95bac4f81b42c486e10b57b5dae04e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/8564 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a30e0ce1053432fb579720f3611f156"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8564 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af9fa7ecddea40bc91bbb9e45faf8c05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"744a5b1230f84dba98ac5eb45c0f8fc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b583cdabe8ea45fe8817840cf9783455"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/289k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04f8d6cb8c2847c2b00b1b6dbb48a55a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/572k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edf1974431d84eb195cbb06d8ec9e47d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/37523 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b8a2d6615524ddbb0f193e94c7d0da0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/5343 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"781804da853b488da1ee13b946361de2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/9178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9eefacdf4534d3a816914dcfa832596"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"Create dataset info json","metadata":{}},{"cell_type":"code","source":"args = { \n    \"nextqa\": {\n        \"file_name\": \"nextqa.json\",\n        \"formatting\": \"sharegpt\",\n        \"columns\": {\n            \"messages\": \"messages\",\n            \"videos\": \"videos\"\n        },\n        \"tags\": {\n            \"role_tag\": \"role\",\n            \"content_tag\": \"content\",\n            \"user_tag\": \"user\",\n            \"assistant_tag\": \"assistant\"\n        }\n    }\n}\nwith open(\"data/dataset_info.json\", \"w\", encoding=\"utf-8\") as f: \n    json.dump(args, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:27:18.371963Z","iopub.execute_input":"2025-04-01T16:27:18.372249Z","iopub.status.idle":"2025-04-01T16:27:18.377199Z","shell.execute_reply.started":"2025-04-01T16:27:18.372228Z","shell.execute_reply":"2025-04-01T16:27:18.376326Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"Create dataset json","metadata":{}},{"cell_type":"code","source":"with open(\"data/nextqa.json\", \"w\", encoding=\"utf-8\") as f: \n    json.dump(dataset, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:27:20.537084Z","iopub.execute_input":"2025-04-01T16:27:20.537388Z","iopub.status.idle":"2025-04-01T16:27:21.664506Z","shell.execute_reply.started":"2025-04-01T16:27:20.537362Z","shell.execute_reply":"2025-04-01T16:27:21.663845Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Fine-tuning Model","metadata":{}},{"cell_type":"markdown","source":"Import packages","metadata":{}},{"cell_type":"code","source":"import json\nimport wandb\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:24:49.301661Z","iopub.execute_input":"2025-04-01T16:24:49.301979Z","iopub.status.idle":"2025-04-01T16:24:51.390940Z","shell.execute_reply.started":"2025-04-01T16:24:49.301954Z","shell.execute_reply":"2025-04-01T16:24:51.390325Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Login to wandb","metadata":{}},{"cell_type":"code","source":"wandb.login(key=UserSecretsClient().get_secret(\"WANDB_API_KEY\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:24:53.090978Z","iopub.execute_input":"2025-04-01T16:24:53.091261Z","iopub.status.idle":"2025-04-01T16:24:59.646622Z","shell.execute_reply.started":"2025-04-01T16:24:53.091239Z","shell.execute_reply":"2025-04-01T16:24:59.645984Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseanjeanmoey123\u001b[0m (\u001b[33mseanjeanmoey123-nus\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"Create fine-tuning script","metadata":{}},{"cell_type":"code","source":"args = {\n    \"model_name_or_path\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n    \"image_max_pixels\": 4096,\n    \"video_max_pixels\": 4096,\n    \"trust_remote_code\": True,\n    \"stage\": \"sft\",\n    \"do_train\": True,\n    \"finetuning_type\": \"lora\",\n    \"deepspeed\": \"examples/deepspeed/ds_z3_offload_config.json\",\n    \"use_fast_tokenizer\": True,\n    \"lora_rank\": 8,\n    \"lora_target\": \"all\",\n    \"dataset\": \"nextqa\",\n    \"template\": \"qwen2_vl\",\n    # \"cutoff_len\": 2048,\n    \"max_samples\": 128,\n    \"overwrite_cache\": True,\n    \"preprocessing_num_workers\": 128,\n    # \"dataloader_num_workers\": 4,\n    \"output_dir\": \"/kaggle/working/finetuned\",\n    \"logging_steps\": 10,\n    \"save_steps\": 500,\n    \"plot_loss\": True,\n    \"overwrite_output_dir\": True,\n    \"save_only_model\": False,\n    \"per_device_train_batch_size\": 1,\n    \"gradient_accumulation_steps\": 8,\n    \"learning_rate\": 1.0e-4,\n    \"num_train_epochs\": 3.0,\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_steps\": 100,\n    \"weight_decay\": 0.1,\n    \"warmup_ratio\": 0.1,\n    \"bf16\": True,\n    \"ddp_timeout\": 180000000,\n    \"resume_from_checkpoint\": None,\n    # \"val_size\": 0.1,\n    # \"per_device_eval_batch_size\": 1,\n    # \"eval_strategy\": \"steps\",\n    # \"eval_steps\": 500,\n}\nwith open(\"train.json\", \"w\", encoding=\"utf-8\") as f: \n    json.dump(args, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:27:29.442541Z","iopub.execute_input":"2025-04-01T16:27:29.442893Z","iopub.status.idle":"2025-04-01T16:27:29.448680Z","shell.execute_reply.started":"2025-04-01T16:27:29.442862Z","shell.execute_reply":"2025-04-01T16:27:29.447843Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"Train model","metadata":{}},{"cell_type":"code","source":"!llamafactory-cli train train.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:27:34.002676Z","iopub.execute_input":"2025-04-01T16:27:34.003017Z","iopub.status.idle":"2025-04-01T16:58:14.337385Z","shell.execute_reply.started":"2025-04-01T16:27:34.002991Z","shell.execute_reply":"2025-04-01T16:58:14.335227Z"}},"outputs":[{"name":"stdout","text":"2025-04-01 16:27:42.833337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-01 16:27:43.052643: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-01 16:27:43.115424: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[2025-04-01 16:27:56,226] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[INFO|2025-04-01 16:28:02] llamafactory.cli:143 >> Initializing 2 distributed tasks at: 127.0.0.1:40901\nW0401 16:28:04.229000 337 torch/distributed/run.py:793] \nW0401 16:28:04.229000 337 torch/distributed/run.py:793] *****************************************\nW0401 16:28:04.229000 337 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0401 16:28:04.229000 337 torch/distributed/run.py:793] *****************************************\n2025-04-01 16:28:09.462278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-01 16:28:09.463703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-01 16:28:09.483475: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-01 16:28:09.485331: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-01 16:28:09.490502: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-04-01 16:28:09.493060: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[2025-04-01 16:28:12,942] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2025-04-01 16:28:12,942] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2025-04-01 16:28:15,300] [INFO] [comm.py:658:init_distributed] cdb=None\n[2025-04-01 16:28:15,300] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n[2025-04-01 16:28:15,307] [INFO] [comm.py:658:init_distributed] cdb=None\n[WARNING|2025-04-01 16:28:15] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n[INFO|2025-04-01 16:28:15] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n[INFO|2025-04-01 16:28:15] llamafactory.hparams.parser:383 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\ntokenizer_config.json: 100%|███████████████| 7.23k/7.23k [00:00<00:00, 32.8MB/s]\nvocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 32.1MB/s]\nmerges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 36.7MB/s]\ntokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 35.5MB/s]\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,233 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,233 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,233 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,234 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,234 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,234 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,234 >> loading file chat_template.jinja from cache at None\n[INFO|tokenization_utils_base.py:2323] 2025-04-01 16:28:16,632 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\npreprocessor_config.json: 100%|████████████████| 350/350 [00:00<00:00, 2.58MB/s]\n[INFO|image_processing_base.py:381] 2025-04-01 16:28:16,789 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n[INFO|image_processing_base.py:381] 2025-04-01 16:28:16,828 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n[WARNING|logging.py:329] 2025-04-01 16:28:16,828 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n[INFO|image_processing_base.py:434] 2025-04-01 16:28:16,829 >> Image processor Qwen2VLImageProcessor {\n  \"do_convert_rgb\": true,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.48145466,\n    0.4578275,\n    0.40821073\n  ],\n  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n  \"image_std\": [\n    0.26862954,\n    0.26130258,\n    0.27577711\n  ],\n  \"max_pixels\": 12845056,\n  \"merge_size\": 2,\n  \"min_pixels\": 3136,\n  \"patch_size\": 14,\n  \"processor_class\": \"Qwen2_5_VLProcessor\",\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"longest_edge\": 12845056,\n    \"shortest_edge\": 3136\n  },\n  \"temporal_patch_size\": 2\n}\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,871 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,871 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,871 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,871 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,871 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,871 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 16:28:16,871 >> loading file chat_template.jinja from cache at None\n[INFO|tokenization_utils_base.py:2323] 2025-04-01 16:28:17,230 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nchat_template.json: 100%|██████████████████| 1.05k/1.05k [00:00<00:00, 6.11MB/s]\n[INFO|processing_utils.py:876] 2025-04-01 16:28:18,060 >> Processor Qwen2_5_VLProcessor:\n- image_processor: Qwen2VLImageProcessor {\n  \"do_convert_rgb\": true,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.48145466,\n    0.4578275,\n    0.40821073\n  ],\n  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n  \"image_std\": [\n    0.26862954,\n    0.26130258,\n    0.27577711\n  ],\n  \"max_pixels\": 12845056,\n  \"merge_size\": 2,\n  \"min_pixels\": 3136,\n  \"patch_size\": 14,\n  \"processor_class\": \"Qwen2_5_VLProcessor\",\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"longest_edge\": 12845056,\n    \"shortest_edge\": 3136\n  },\n  \"temporal_patch_size\": 2\n}\n\n- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n}\n)\n\n{\n  \"processor_class\": \"Qwen2_5_VLProcessor\"\n}\n\n[INFO|2025-04-01 16:28:18] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n[INFO|2025-04-01 16:28:18] llamafactory.data.loader:143 >> Loading dataset nextqa.json...\n[rank1]:[W401 16:28:18.022925180 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\nSetting num_proc from 128 back to 1 for the train split to disable multiprocessing as it only contains one shard.\nGenerating train split: 60608 examples [00:00, 61256.85 examples/s]\nConverting format of dataset (num_proc=128): 100%|█| 128/128 [00:02<00:00, 57.47\n[rank0]:[W401 16:28:25.102288122 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\nRunning tokenizer on dataset (num_proc=128): 100%|█| 128/128 [02:08<00:00,  1.01\ntraining example:\ninput_ids:\n[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151653, 12555, 572, 279, 8171, 304, 6303, 3730, 438, 279, 8171, 304, 2518, 15391, 279, 7406, 28412, 198, 32, 13, 11699, 304, 279, 7406, 198, 33, 13, 3271, 806, 6078, 311, 1008, 10496, 198, 34, 13, 1459, 518, 279, 26763, 198, 35, 13, 2182, 289, 80535, 1119, 10780, 198, 36, 13, 25367, 705, 279, 31149, 198, 3379, 825, 1850, 4226, 311, 279, 3403, 5248, 62626, 3405, 3118, 389, 279, 2766, 13, 39533, 448, 1172, 279, 6524, 320, 32, 11, 425, 11, 356, 11, 422, 476, 468, 8, 315, 279, 4396, 2999, 13, 151645, 198, 151644, 77091, 198, 32, 151645, 198]\ninputs:\n<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|vision_end|>what was the boy in blue doing as the boy in red pushed the cart backwards\nA. sitting in the cart\nB. move his hands to other chair\nC. point at the trucks\nD. put waffles into mouth\nE. climb up the stairs\nSelect one best answer to the above multiple-choice question based on the video. Respond with only the letter (A, B, C, D or E) of the correct option.<|im_end|>\n<|im_start|>assistant\nA<|im_end|>\n\nlabel_ids:\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 32, 151645, 198]\nlabels:\nA<|im_end|>\n\nconfig.json: 100%|█████████████████████████| 1.37k/1.37k [00:00<00:00, 9.24MB/s]\n[INFO|configuration_utils.py:699] 2025-04-01 16:30:39,802 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/config.json\n[INFO|configuration_utils.py:771] 2025-04-01 16:30:39,815 >> Model config Qwen2_5_VLConfig {\n  \"architectures\": [\n    \"Qwen2_5_VLForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"image_token_id\": 151655,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 128000,\n  \"max_window_layers\": 70,\n  \"model_type\": \"qwen2_5_vl\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": {\n    \"mrope_section\": [\n      16,\n      24,\n      24\n    ],\n    \"rope_type\": \"default\",\n    \"type\": \"default\"\n  },\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.50.0\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"video_token_id\": 151656,\n  \"vision_config\": {\n    \"depth\": 32,\n    \"fullatt_block_indexes\": [\n      7,\n      15,\n      23,\n      31\n    ],\n    \"hidden_act\": \"silu\",\n    \"hidden_size\": 1280,\n    \"in_channels\": 3,\n    \"in_chans\": 3,\n    \"intermediate_size\": 3420,\n    \"model_type\": \"qwen2_5_vl\",\n    \"num_heads\": 16,\n    \"out_hidden_size\": 2048,\n    \"patch_size\": 14,\n    \"spatial_merge_size\": 2,\n    \"spatial_patch_size\": 14,\n    \"temporal_patch_size\": 2,\n    \"tokens_per_second\": 2,\n    \"window_size\": 112\n  },\n  \"vision_end_token_id\": 151653,\n  \"vision_start_token_id\": 151652,\n  \"vision_token_id\": 151654,\n  \"vocab_size\": 151936\n}\n\n[INFO|2025-04-01 16:30:39] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\nmodel.safetensors.index.json: 100%|█████████| 65.4k/65.4k [00:00<00:00, 165MB/s]\n[INFO|modeling_utils.py:1154] 2025-04-01 16:30:40,313 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/model.safetensors.index.json\nFetching 2 files:   0%|                                   | 0/2 [00:00<?, ?it/s]\nmodel-00002-of-00002.safetensors:   0%|             | 0.00/3.53G [00:00<?, ?B/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   0%|             | 0.00/3.98G [00:00<?, ?B/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:   1%|     | 21.0M/3.53G [00:00<00:27, 129MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   0%|    | 10.5M/3.98G [00:00<00:59, 66.9MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:   1%|     | 52.4M/3.53G [00:00<00:18, 192MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   1%|     | 31.5M/3.98G [00:00<00:34, 116MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:   2%|     | 83.9M/3.53G [00:00<00:16, 214MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   2%|     | 62.9M/3.98G [00:00<00:25, 155MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:   3%|▏     | 115M/3.53G [00:00<00:14, 229MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   2%|     | 94.4M/3.98G [00:00<00:21, 184MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:   4%|▏     | 147M/3.53G [00:00<00:14, 233MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   3%|▏     | 126M/3.98G [00:00<00:18, 206MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:   5%|▎     | 178M/3.53G [00:00<00:14, 237MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   4%|▏     | 157M/3.98G [00:00<00:17, 218MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:   6%|▎     | 210M/3.53G [00:00<00:13, 240MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   5%|▎     | 189M/3.98G [00:00<00:16, 231MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:   7%|▍     | 241M/3.53G [00:01<00:13, 242MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   6%|▎     | 220M/3.98G [00:01<00:15, 236MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:   8%|▍     | 273M/3.53G [00:01<00:13, 242MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   6%|▍     | 252M/3.98G [00:01<00:15, 238MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:   9%|▌     | 304M/3.53G [00:01<00:13, 240MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   7%|▍     | 283M/3.98G [00:01<00:15, 241MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  10%|▌     | 336M/3.53G [00:01<00:13, 238MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   8%|▍     | 315M/3.98G [00:01<00:15, 237MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  10%|▌     | 367M/3.53G [00:01<00:13, 237MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   9%|▌     | 346M/3.98G [00:01<00:15, 235MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  11%|▋     | 398M/3.53G [00:01<00:13, 239MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:   9%|▌     | 377M/3.98G [00:01<00:15, 238MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  12%|▋     | 430M/3.53G [00:01<00:12, 239MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  10%|▌     | 409M/3.98G [00:01<00:14, 239MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  13%|▊     | 461M/3.53G [00:01<00:12, 240MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  11%|▋     | 440M/3.98G [00:02<00:14, 239MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  14%|▊     | 493M/3.53G [00:02<00:12, 243MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  12%|▋     | 472M/3.98G [00:02<00:14, 238MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  15%|▉     | 524M/3.53G [00:02<00:12, 240MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  13%|▊     | 503M/3.98G [00:02<00:14, 240MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  16%|▉     | 556M/3.53G [00:02<00:12, 239MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  13%|▊     | 535M/3.98G [00:02<00:14, 231MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  17%|▉     | 587M/3.53G [00:02<00:12, 241MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  14%|▊     | 566M/3.98G [00:02<00:14, 236MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  18%|█     | 619M/3.53G [00:02<00:11, 246MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  15%|▉     | 598M/3.98G [00:02<00:14, 237MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  18%|█     | 650M/3.53G [00:02<00:11, 245MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  16%|▉     | 629M/3.98G [00:02<00:14, 239MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  19%|█▏    | 682M/3.53G [00:02<00:11, 243MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  17%|▉     | 661M/3.98G [00:02<00:13, 241MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  20%|█▏    | 713M/3.53G [00:03<00:11, 241MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  17%|█     | 692M/3.98G [00:03<00:13, 243MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  21%|█▎    | 744M/3.53G [00:03<00:11, 241MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  18%|█     | 724M/3.98G [00:03<00:13, 244MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  22%|█▎    | 776M/3.53G [00:03<00:11, 242MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  19%|█▏    | 755M/3.98G [00:03<00:13, 244MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  23%|█▎    | 807M/3.53G [00:03<00:11, 242MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  20%|█▏    | 786M/3.98G [00:03<00:13, 244MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  24%|█▍    | 839M/3.53G [00:03<00:11, 240MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  21%|█▏    | 818M/3.98G [00:03<00:12, 249MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  25%|█▍    | 870M/3.53G [00:03<00:11, 230MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  21%|█▎    | 849M/3.98G [00:03<00:12, 248MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  26%|█▌    | 902M/3.53G [00:03<00:11, 235MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  22%|█▎    | 881M/3.98G [00:03<00:12, 250MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  26%|█▌    | 933M/3.53G [00:03<00:10, 238MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  23%|█▎    | 912M/3.98G [00:03<00:12, 245MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  27%|█▋    | 965M/3.53G [00:04<00:10, 242MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  24%|█▍    | 944M/3.98G [00:04<00:12, 243MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  28%|█▋    | 996M/3.53G [00:04<00:10, 242MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  24%|█▍    | 975M/3.98G [00:04<00:12, 234MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  29%|█▍   | 1.03G/3.53G [00:04<00:10, 240MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  25%|█▎   | 1.01G/3.98G [00:04<00:12, 232MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  30%|█▌   | 1.06G/3.53G [00:04<00:10, 238MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  26%|█▎   | 1.04G/3.98G [00:04<00:12, 233MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  31%|█▌   | 1.09G/3.53G [00:04<00:10, 237MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  27%|█▎   | 1.07G/3.98G [00:04<00:12, 237MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  32%|█▌   | 1.12G/3.53G [00:04<00:10, 240MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  28%|█▍   | 1.10G/3.98G [00:04<00:12, 235MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  33%|█▋   | 1.15G/3.53G [00:04<00:09, 243MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  28%|█▍   | 1.13G/3.98G [00:04<00:11, 239MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  34%|█▋   | 1.18G/3.53G [00:04<00:09, 245MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  29%|█▍   | 1.16G/3.98G [00:05<00:11, 237MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  34%|█▋   | 1.22G/3.53G [00:05<00:09, 249MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  30%|█▌   | 1.20G/3.98G [00:05<00:11, 235MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  35%|█▊   | 1.25G/3.53G [00:05<00:09, 251MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  31%|█▌   | 1.23G/3.98G [00:05<00:11, 238MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  36%|█▊   | 1.28G/3.53G [00:05<00:09, 245MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  32%|█▌   | 1.26G/3.98G [00:05<00:11, 246MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  32%|█▌   | 1.29G/3.98G [00:05<00:10, 253MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  37%|█▊   | 1.31G/3.53G [00:05<00:10, 218MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  38%|█▉   | 1.34G/3.53G [00:05<00:09, 226MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  33%|█▋   | 1.32G/3.98G [00:05<00:10, 245MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  39%|█▉   | 1.37G/3.53G [00:05<00:09, 232MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  34%|█▋   | 1.35G/3.98G [00:05<00:10, 241MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  40%|█▉   | 1.41G/3.53G [00:05<00:08, 236MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  35%|█▋   | 1.38G/3.98G [00:05<00:10, 241MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  41%|██   | 1.44G/3.53G [00:06<00:08, 233MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  36%|█▊   | 1.42G/3.98G [00:06<00:11, 233MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  42%|██   | 1.47G/3.53G [00:06<00:09, 229MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  36%|█▊   | 1.45G/3.98G [00:06<00:11, 230MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  43%|██▏  | 1.50G/3.53G [00:06<00:08, 227MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  37%|█▊   | 1.48G/3.98G [00:06<00:10, 230MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  43%|██▏  | 1.53G/3.53G [00:06<00:08, 225MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  38%|█▉   | 1.51G/3.98G [00:06<00:10, 226MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  44%|██▏  | 1.56G/3.53G [00:06<00:08, 230MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  39%|█▉   | 1.54G/3.98G [00:06<00:10, 224MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  45%|██▎  | 1.59G/3.53G [00:06<00:08, 229MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  39%|█▉   | 1.57G/3.98G [00:06<00:11, 218MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  46%|██▎  | 1.63G/3.53G [00:06<00:08, 231MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  40%|██   | 1.60G/3.98G [00:06<00:10, 220MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  47%|██▎  | 1.66G/3.53G [00:07<00:08, 233MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  41%|██   | 1.64G/3.98G [00:07<00:10, 221MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  48%|██▍  | 1.69G/3.53G [00:07<00:07, 231MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  42%|██   | 1.67G/3.98G [00:07<00:10, 222MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  49%|██▍  | 1.72G/3.53G [00:07<00:07, 227MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  43%|██▏  | 1.70G/3.98G [00:07<00:10, 226MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  50%|██▍  | 1.75G/3.53G [00:07<00:07, 228MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  43%|██▏  | 1.73G/3.98G [00:07<00:09, 229MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  51%|██▌  | 1.78G/3.53G [00:07<00:07, 226MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  44%|██▏  | 1.76G/3.98G [00:07<00:09, 224MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  51%|██▌  | 1.81G/3.53G [00:07<00:07, 228MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  45%|██▎  | 1.79G/3.98G [00:07<00:09, 225MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  52%|██▌  | 1.85G/3.53G [00:07<00:07, 225MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  46%|██▎  | 1.82G/3.98G [00:07<00:09, 227MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  53%|██▋  | 1.88G/3.53G [00:07<00:07, 229MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  47%|██▎  | 1.86G/3.98G [00:08<00:09, 228MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  54%|██▋  | 1.91G/3.53G [00:08<00:07, 228MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  47%|██▎  | 1.89G/3.98G [00:08<00:09, 224MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  55%|██▊  | 1.94G/3.53G [00:08<00:07, 227MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  48%|██▍  | 1.92G/3.98G [00:08<00:09, 224MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  56%|██▊  | 1.97G/3.53G [00:08<00:06, 224MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  49%|██▍  | 1.95G/3.98G [00:08<00:09, 224MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  57%|██▊  | 2.00G/3.53G [00:08<00:06, 224MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  50%|██▍  | 1.98G/3.98G [00:08<00:08, 223MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  58%|██▉  | 2.03G/3.53G [00:08<00:06, 220MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  51%|██▌  | 2.01G/3.98G [00:08<00:08, 228MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  59%|██▉  | 2.07G/3.53G [00:08<00:06, 224MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  51%|██▌  | 2.04G/3.98G [00:08<00:08, 227MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  59%|██▉  | 2.10G/3.53G [00:08<00:06, 226MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  52%|██▌  | 2.08G/3.98G [00:09<00:08, 227MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  60%|███  | 2.13G/3.53G [00:09<00:06, 228MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  53%|██▋  | 2.11G/3.98G [00:09<00:08, 226MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  61%|███  | 2.16G/3.53G [00:09<00:05, 229MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  54%|██▋  | 2.14G/3.98G [00:09<00:08, 227MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  62%|███  | 2.19G/3.53G [00:09<00:05, 228MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  55%|██▋  | 2.17G/3.98G [00:09<00:07, 231MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  63%|███▏ | 2.22G/3.53G [00:09<00:05, 229MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  55%|██▊  | 2.20G/3.98G [00:09<00:07, 231MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  64%|███▏ | 2.25G/3.53G [00:09<00:05, 233MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  56%|██▊  | 2.23G/3.98G [00:09<00:07, 237MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  65%|███▏ | 2.29G/3.53G [00:09<00:05, 236MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  57%|██▊  | 2.26G/3.98G [00:09<00:07, 234MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  66%|███▎ | 2.32G/3.53G [00:09<00:05, 239MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  58%|██▉  | 2.30G/3.98G [00:09<00:07, 229MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  67%|███▎ | 2.35G/3.53G [00:10<00:04, 238MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  58%|██▉  | 2.33G/3.98G [00:10<00:07, 207MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  67%|███▎ | 2.38G/3.53G [00:10<00:04, 238MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  68%|███▍ | 2.41G/3.53G [00:10<00:04, 249MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  69%|███▍ | 2.44G/3.53G [00:10<00:04, 253MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  59%|██▉  | 2.36G/3.98G [00:10<00:10, 162MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  70%|███▌ | 2.47G/3.53G [00:10<00:04, 252MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  71%|███▌ | 2.51G/3.53G [00:10<00:04, 254MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  60%|██▉  | 2.38G/3.98G [00:10<00:12, 131MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  72%|███▌ | 2.54G/3.53G [00:10<00:03, 259MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  60%|███  | 2.40G/3.98G [00:10<00:11, 135MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  73%|███▋ | 2.57G/3.53G [00:10<00:03, 257MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  74%|███▋ | 2.60G/3.53G [00:11<00:03, 263MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  61%|███  | 2.42G/3.98G [00:11<00:12, 124MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  75%|███▋ | 2.63G/3.53G [00:11<00:03, 253MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  61%|███  | 2.44G/3.98G [00:11<00:11, 130MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  76%|███▊ | 2.66G/3.53G [00:11<00:03, 221MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  62%|███  | 2.46G/3.98G [00:11<00:10, 140MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  76%|███▊ | 2.69G/3.53G [00:11<00:03, 223MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  77%|███▊ | 2.73G/3.53G [00:11<00:03, 231MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  62%|███  | 2.49G/3.98G [00:11<00:14, 106MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  78%|███▉ | 2.76G/3.53G [00:11<00:03, 237MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  63%|███▏ | 2.51G/3.98G [00:11<00:12, 117MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  79%|███▉ | 2.79G/3.53G [00:11<00:03, 238MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  63%|███▏ | 2.53G/3.98G [00:11<00:11, 129MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  80%|███▉ | 2.82G/3.53G [00:11<00:02, 237MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  64%|███▏ | 2.55G/3.98G [00:12<00:11, 128MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  81%|████ | 2.85G/3.53G [00:12<00:02, 238MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  65%|███▏ | 2.58G/3.98G [00:12<00:08, 157MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  82%|████ | 2.88G/3.53G [00:12<00:02, 240MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  83%|████▏| 2.92G/3.53G [00:12<00:02, 245MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  66%|███▎ | 2.61G/3.98G [00:12<00:09, 143MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  84%|████▏| 2.95G/3.53G [00:12<00:02, 248MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  84%|████▏| 2.98G/3.53G [00:12<00:02, 248MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  66%|███▎ | 2.64G/3.98G [00:12<00:09, 138MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  85%|████▎| 3.01G/3.53G [00:12<00:02, 249MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  86%|████▎| 3.04G/3.53G [00:12<00:01, 250MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  87%|████▎| 3.07G/3.53G [00:12<00:01, 251MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  88%|████▍| 3.10G/3.53G [00:13<00:01, 252MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  67%|██▋ | 2.66G/3.98G [00:13<00:13, 97.2MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  89%|████▍| 3.14G/3.53G [00:13<00:01, 255MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  90%|████▍| 3.17G/3.53G [00:13<00:01, 256MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  67%|██▋ | 2.68G/3.98G [00:13<00:14, 90.3MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  91%|████▌| 3.20G/3.53G [00:13<00:01, 249MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  68%|███▍ | 2.71G/3.98G [00:13<00:12, 104MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  92%|████▌| 3.23G/3.53G [00:13<00:01, 248MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  69%|███▍ | 2.74G/3.98G [00:13<00:10, 122MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  92%|████▌| 3.26G/3.53G [00:13<00:01, 248MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  70%|███▍ | 2.77G/3.98G [00:13<00:08, 144MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  93%|████▋| 3.29G/3.53G [00:13<00:00, 243MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  94%|████▋| 3.32G/3.53G [00:13<00:00, 255MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  70%|███▌ | 2.79G/3.98G [00:14<00:09, 122MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  95%|████▊| 3.36G/3.53G [00:14<00:00, 248MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  96%|████▊| 3.39G/3.53G [00:14<00:00, 247MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  71%|███▌ | 2.82G/3.98G [00:14<00:09, 126MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  97%|████▊| 3.42G/3.53G [00:14<00:00, 244MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  98%|████▉| 3.45G/3.53G [00:14<00:00, 249MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  71%|███▌ | 2.84G/3.98G [00:14<00:09, 115MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors:  99%|████▉| 3.48G/3.53G [00:14<00:00, 249MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  72%|███▌ | 2.86G/3.98G [00:14<00:09, 119MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors: 100%|████▉| 3.51G/3.53G [00:14<00:00, 250MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors: 100%|█████| 3.53G/3.53G [00:14<00:00, 237MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00002.safetensors:  73%|███▋ | 2.90G/3.98G [00:14<00:08, 129MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  74%|███▋ | 2.94G/3.98G [00:15<00:08, 128MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  74%|███▋ | 2.96G/3.98G [00:15<00:10, 100MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  75%|██▉ | 2.98G/3.98G [00:15<00:10, 99.6MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  75%|███ | 3.00G/3.98G [00:16<00:11, 88.6MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  76%|███▊ | 3.02G/3.98G [00:16<00:09, 102MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  77%|███▊ | 3.05G/3.98G [00:16<00:07, 129MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  77%|███▊ | 3.08G/3.98G [00:16<00:05, 154MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  78%|███▉ | 3.11G/3.98G [00:16<00:04, 175MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  79%|███▉ | 3.15G/3.98G [00:16<00:04, 194MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  80%|███▉ | 3.18G/3.98G [00:16<00:03, 206MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  81%|████ | 3.21G/3.98G [00:17<00:03, 214MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  81%|████ | 3.24G/3.98G [00:17<00:03, 222MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  82%|████ | 3.27G/3.98G [00:17<00:03, 228MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  83%|████▏| 3.30G/3.98G [00:17<00:02, 233MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  84%|████▏| 3.33G/3.98G [00:17<00:02, 235MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  85%|████▏| 3.37G/3.98G [00:17<00:02, 236MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  85%|████▎| 3.40G/3.98G [00:17<00:02, 231MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  86%|████▎| 3.43G/3.98G [00:17<00:02, 237MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  87%|████▎| 3.46G/3.98G [00:18<00:02, 238MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  88%|████▍| 3.49G/3.98G [00:18<00:02, 242MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  88%|████▍| 3.52G/3.98G [00:18<00:01, 243MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  89%|████▍| 3.55G/3.98G [00:18<00:01, 243MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  90%|████▌| 3.59G/3.98G [00:18<00:01, 240MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  91%|████▌| 3.62G/3.98G [00:18<00:01, 236MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  92%|████▌| 3.65G/3.98G [00:18<00:01, 236MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  92%|████▌| 3.68G/3.98G [00:18<00:01, 236MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  93%|████▋| 3.71G/3.98G [00:19<00:01, 242MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  94%|████▋| 3.74G/3.98G [00:19<00:00, 241MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  95%|████▋| 3.77G/3.98G [00:19<00:00, 241MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  96%|████▊| 3.81G/3.98G [00:19<00:00, 237MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  96%|████▊| 3.84G/3.98G [00:19<00:00, 234MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  97%|████▊| 3.87G/3.98G [00:19<00:00, 232MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  98%|████▉| 3.90G/3.98G [00:19<00:00, 235MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  99%|████▉| 3.93G/3.98G [00:20<00:00, 239MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors: 100%|█████| 3.98G/3.98G [00:20<00:00, 197MB/s]\u001b[A\u001b[A\nFetching 2 files: 100%|███████████████████████████| 2/2 [00:20<00:00, 10.23s/it]\n[INFO|modeling_utils.py:3747] 2025-04-01 16:31:00,816 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n[2025-04-01 16:31:00,817] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2\n[INFO|configuration_utils.py:1139] 2025-04-01 16:31:00,830 >> Generate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"use_cache\": false\n}\n\n[INFO|modeling_utils.py:2170] 2025-04-01 16:31:00,831 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.\nFetching 2 files: 100%|███████████████████████████| 2/2 [00:20<00:00, 10.19s/it]\n[2025-04-01 16:31:00,865] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2\n[2025-04-01 16:31:08,856] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 825, num_elems = 4.07B\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:21<00:00, 10.93s/it]\ngeneration_config.json: 100%|██████████████████| 216/216 [00:00<00:00, 1.28MB/s]\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:21<00:00, 11.00s/it]\n[INFO|modeling_utils.py:4987] 2025-04-01 16:31:31,278 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.\n\n[INFO|modeling_utils.py:4995] 2025-04-01 16:31:31,278 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-3B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1094] 2025-04-01 16:31:31,328 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/generation_config.json\n[INFO|configuration_utils.py:1139] 2025-04-01 16:31:31,329 >> Generate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.05,\n  \"temperature\": 1e-06\n}\n\n[INFO|2025-04-01 16:31:31] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n[INFO|2025-04-01 16:31:31] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n[INFO|2025-04-01 16:31:31] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.\n[INFO|2025-04-01 16:31:31] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n[INFO|2025-04-01 16:31:31] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,gate_proj,o_proj,v_proj,k_proj,up_proj,down_proj\n[INFO|2025-04-01 16:31:31] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['visual.patch_embed', 'visual.blocks'].\n[INFO|2025-04-01 16:31:31] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: visual.merger.\n[INFO|2025-04-01 16:31:31] llamafactory.model.loader:143 >> trainable params: 14,966,784 || all params: 3,769,589,760 || trainable%: 0.3970\n[INFO|trainer.py:748] 2025-04-01 16:31:32,044 >> Using auto half precision backend\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n[WARNING|trainer.py:783] 2025-04-01 16:31:32,045 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n[INFO|deepspeed.py:386] 2025-04-01 16:31:32,541 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)\nInstalled CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/py310_cu121/cpu_adam...\nInstalled CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...\nBuilding extension module cpu_adam...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o \n[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o cpu_adam.so\nLoading extension module cpu_adam...\nTime to load cpu_adam op: 40.21089792251587 seconds\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\nConfig: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n[2025-04-01 16:32:14,174] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown\n[2025-04-01 16:32:14,174] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2\nLoading extension module cpu_adam...\nTime to load cpu_adam op: 40.26385188102722 seconds\n[2025-04-01 16:32:14,246] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n[2025-04-01 16:32:14,252] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n[2025-04-01 16:32:14,253] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n[2025-04-01 16:32:14,341] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n[2025-04-01 16:32:14,341] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n[2025-04-01 16:32:14,341] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n[2025-04-01 16:32:14,341] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\n[2025-04-01 16:32:14,738] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n[2025-04-01 16:32:14,739] [INFO] [utils.py:782:see_memory_usage] MA 0.03 GB         Max_MA 1.74 GB         CA 0.06 GB         Max_CA 2 GB \n[2025-04-01 16:32:14,739] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.15 GB, percent = 54.7%\n[2025-04-01 16:32:14,752] [INFO] [stage3.py:170:__init__] Reduce bucket size 4194304\n[2025-04-01 16:32:14,752] [INFO] [stage3.py:171:__init__] Prefetch bucket size 3774873\n[2025-04-01 16:32:15,144] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n[2025-04-01 16:32:15,145] [INFO] [utils.py:782:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 0.06 GB         Max_CA 0 GB \n[2025-04-01 16:32:15,145] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.15 GB, percent = 54.7%\nParameter Offload: Total persistent parameters: 6211584 in 804 params\n[2025-04-01 16:32:16,704] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n[2025-04-01 16:32:16,705] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.03 GB         CA 0.06 GB         Max_CA 0 GB \n[2025-04-01 16:32:16,705] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.26 GB, percent = 55.0%\n[2025-04-01 16:32:17,164] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n[2025-04-01 16:32:17,164] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n[2025-04-01 16:32:17,165] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.26 GB, percent = 55.0%\n[2025-04-01 16:32:17,661] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1\n[2025-04-01 16:32:17,661] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n[2025-04-01 16:32:17,662] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.33 GB, percent = 55.3%\n[2025-04-01 16:32:18,129] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n[2025-04-01 16:32:18,130] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n[2025-04-01 16:32:18,130] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.33 GB, percent = 55.3%\n[2025-04-01 16:32:18,644] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n[2025-04-01 16:32:18,645] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n[2025-04-01 16:32:18,645] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.37 GB, percent = 55.4%\n[2025-04-01 16:32:19,120] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n[2025-04-01 16:32:19,121] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n[2025-04-01 16:32:19,121] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.42 GB, percent = 55.6%\n[2025-04-01 16:32:19,679] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n[2025-04-01 16:32:19,680] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n[2025-04-01 16:32:19,680] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.47 GB, percent = 55.7%\n[2025-04-01 16:32:19,681] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized\n[2025-04-01 16:32:20,486] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n[2025-04-01 16:32:20,487] [INFO] [utils.py:782:see_memory_usage] MA 0.01 GB         Max_MA 0.01 GB         CA 0.08 GB         Max_CA 0 GB \n[2025-04-01 16:32:20,487] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.54 GB, percent = 55.9%\n[2025-04-01 16:32:20,488] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\n[2025-04-01 16:32:20,488] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n[2025-04-01 16:32:20,488] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n[2025-04-01 16:32:20,488] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n[2025-04-01 16:32:20,500] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:\n[2025-04-01 16:32:20,500] [INFO] [config.py:1004:print]   activation_checkpointing_config  {\n    \"partition_activations\": false, \n    \"contiguous_memory_optimization\": false, \n    \"cpu_checkpointing\": false, \n    \"number_checkpoints\": null, \n    \"synchronize_checkpoint_boundary\": false, \n    \"profile\": false\n}\n[2025-04-01 16:32:20,500] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n[2025-04-01 16:32:20,500] [INFO] [config.py:1004:print]   amp_enabled .................. False\n[2025-04-01 16:32:20,500] [INFO] [config.py:1004:print]   amp_params ................... False\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   autotuning_config ............ {\n    \"enabled\": false, \n    \"start_step\": null, \n    \"end_step\": null, \n    \"metric_path\": null, \n    \"arg_mappings\": null, \n    \"metric\": \"throughput\", \n    \"model_info\": null, \n    \"results_dir\": \"autotuning_results\", \n    \"exps_dir\": \"autotuning_exps\", \n    \"overwrite\": true, \n    \"fast\": true, \n    \"start_profile_step\": 3, \n    \"end_profile_step\": 5, \n    \"tuner_type\": \"gridsearch\", \n    \"tuner_early_stopping\": 5, \n    \"tuner_num_trials\": 50, \n    \"model_info_path\": null, \n    \"mp_size\": 1, \n    \"max_train_batch_size\": null, \n    \"min_train_batch_size\": 1, \n    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n    \"min_train_micro_batch_size_per_gpu\": 1, \n    \"num_tuning_micro_batch_sizes\": 3\n}\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   bfloat16_enabled ............. True\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  True\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x79f7786deec0>\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   communication_data_type ...... None\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   disable_allgather ............ False\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   dump_state ................... False\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... None\n[2025-04-01 16:32:20,501] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   elasticity_enabled ........... False\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   flops_profiler_config ........ {\n    \"enabled\": false, \n    \"recompute_fwd_factor\": 0.0, \n    \"profile_step\": 1, \n    \"module_depth\": -1, \n    \"top_modules\": 1, \n    \"detailed\": true, \n    \"output_file\": null\n}\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   fp16_auto_cast ............... None\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   fp16_enabled ................. False\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   global_rank .................. 0\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 8\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   graph_harvesting ............. False\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 1\n[2025-04-01 16:32:20,502] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   loss_scale ................... 1.0\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   memory_breakdown ............. False\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   mics_shard_size .............. -1\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   nebula_config ................ {\n    \"enabled\": false, \n    \"persistent_storage_path\": null, \n    \"persistent_time_interval\": 100, \n    \"num_of_version_in_retention\": 2, \n    \"enable_nebula_load\": true, \n    \"load_path\": null\n}\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   optimizer_name ............... None\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   optimizer_params ............. None\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   pld_enabled .................. False\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   pld_params ................... False\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   prescale_gradients ........... False\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   scheduler_name ............... None\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   scheduler_params ............. None\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   sparse_attention ............. None\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   steps_per_print .............. inf\n[2025-04-01 16:32:20,503] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   train_batch_size ............. 16\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  1\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   use_node_local_storage ....... False\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   weight_quantization_config ... None\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   world_size ................... 2\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  True\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=4194304 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=3774873 param_persistence_threshold=20480 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   zero_enabled ................. True\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True\n[2025-04-01 16:32:20,504] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 3\n[2025-04-01 16:32:20,504] [INFO] [config.py:990:print_user_config]   json = {\n    \"train_batch_size\": 16, \n    \"train_micro_batch_size_per_gpu\": 1, \n    \"gradient_accumulation_steps\": 8, \n    \"gradient_clipping\": 1.0, \n    \"zero_allow_untested_optimizer\": true, \n    \"fp16\": {\n        \"enabled\": false, \n        \"loss_scale\": 0, \n        \"loss_scale_window\": 1000, \n        \"initial_scale_power\": 16, \n        \"hysteresis\": 2, \n        \"min_loss_scale\": 1\n    }, \n    \"bf16\": {\n        \"enabled\": true\n    }, \n    \"zero_optimization\": {\n        \"stage\": 3, \n        \"offload_optimizer\": {\n            \"device\": \"cpu\", \n            \"pin_memory\": true\n        }, \n        \"offload_param\": {\n            \"device\": \"cpu\", \n            \"pin_memory\": true\n        }, \n        \"overlap_comm\": false, \n        \"contiguous_gradients\": true, \n        \"sub_group_size\": 1.000000e+09, \n        \"reduce_bucket_size\": 4.194304e+06, \n        \"stage3_prefetch_bucket_size\": 3.774873e+06, \n        \"stage3_param_persistence_threshold\": 2.048000e+04, \n        \"stage3_max_live_parameters\": 1.000000e+09, \n        \"stage3_max_reuse_distance\": 1.000000e+09, \n        \"stage3_gather_16bit_weights_on_model_save\": true\n    }, \n    \"steps_per_print\": inf\n}\n[INFO|trainer.py:2409] 2025-04-01 16:32:20,506 >> ***** Running training *****\n[INFO|trainer.py:2410] 2025-04-01 16:32:20,506 >>   Num examples = 128\n[INFO|trainer.py:2411] 2025-04-01 16:32:20,506 >>   Num Epochs = 3\n[INFO|trainer.py:2412] 2025-04-01 16:32:20,507 >>   Instantaneous batch size per device = 1\n[INFO|trainer.py:2415] 2025-04-01 16:32:20,507 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n[INFO|trainer.py:2416] 2025-04-01 16:32:20,507 >>   Gradient Accumulation steps = 8\n[INFO|trainer.py:2417] 2025-04-01 16:32:20,507 >>   Total optimization steps = 24\n[INFO|trainer.py:2418] 2025-04-01 16:32:20,515 >>   Number of trainable parameters = 14,966,784\n[INFO|integration_utils.py:831] 2025-04-01 16:32:20,530 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseanjeanmoey123\u001b[0m (\u001b[33mseanjeanmoey123-nus\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/LLaMA-Factory/wandb/run-20250401_163220-qf3b4hgi\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/kaggle/working/finetuned\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/seanjeanmoey123-nus/llamafactory\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/seanjeanmoey123-nus/llamafactory/runs/qf3b4hgi\u001b[0m\n{'loss': 0.3206, 'grad_norm': 0.8566879630088806, 'learning_rate': 5e-06, 'epoch': 1.25}\n{'loss': 0.2668, 'grad_norm': 0.6678802967071533, 'learning_rate': 1e-05, 'epoch': 2.5}\n100%|███████████████████████████████████████████| 24/24 [25:15<00:00, 63.25s/it][INFO|trainer.py:3966] 2025-04-01 16:57:48,726 >> Saving model checkpoint to /kaggle/working/finetuned/checkpoint-24\n[INFO|configuration_utils.py:699] 2025-04-01 16:57:49,222 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/config.json\n[INFO|configuration_utils.py:771] 2025-04-01 16:57:49,226 >> Model config Qwen2_5_VLConfig {\n  \"architectures\": [\n    \"Qwen2_5_VLForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"image_token_id\": 151655,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 128000,\n  \"max_window_layers\": 70,\n  \"model_type\": \"qwen2_5_vl\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": {\n    \"mrope_section\": [\n      16,\n      24,\n      24\n    ],\n    \"rope_type\": \"default\",\n    \"type\": \"default\"\n  },\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.50.0\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"video_token_id\": 151656,\n  \"vision_config\": {\n    \"depth\": 32,\n    \"fullatt_block_indexes\": [\n      7,\n      15,\n      23,\n      31\n    ],\n    \"hidden_act\": \"silu\",\n    \"hidden_size\": 1280,\n    \"in_channels\": 3,\n    \"in_chans\": 3,\n    \"intermediate_size\": 3420,\n    \"model_type\": \"qwen2_5_vl\",\n    \"num_heads\": 16,\n    \"out_hidden_size\": 2048,\n    \"patch_size\": 14,\n    \"spatial_merge_size\": 2,\n    \"spatial_patch_size\": 14,\n    \"temporal_patch_size\": 2,\n    \"tokens_per_second\": 2,\n    \"window_size\": 112\n  },\n  \"vision_end_token_id\": 151653,\n  \"vision_start_token_id\": 151652,\n  \"vision_token_id\": 151654,\n  \"vocab_size\": 151936\n}\n\n[INFO|tokenization_utils_base.py:2510] 2025-04-01 16:57:49,319 >> tokenizer config file saved in /kaggle/working/finetuned/checkpoint-24/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-04-01 16:57:49,320 >> Special tokens file saved in /kaggle/working/finetuned/checkpoint-24/special_tokens_map.json\n[2025-04-01 16:57:50,267] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step24 is about to be saved!\n[2025-04-01 16:57:50,318] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/finetuned/checkpoint-24/global_step24/zero_pp_rank_0_mp_rank_00_model_states.pt\n[2025-04-01 16:57:50,318] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/finetuned/checkpoint-24/global_step24/zero_pp_rank_0_mp_rank_00_model_states.pt...\n[2025-04-01 16:57:50,356] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/finetuned/checkpoint-24/global_step24/zero_pp_rank_0_mp_rank_00_model_states.pt.\n[2025-04-01 16:57:50,358] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/finetuned/checkpoint-24/global_step24/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2025-04-01 16:57:50,457] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/finetuned/checkpoint-24/global_step24/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2025-04-01 16:57:50,469] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/finetuned/checkpoint-24/global_step24/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2025-04-01 16:57:50,548] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24 is ready now!\n[INFO|image_processing_base.py:261] 2025-04-01 16:57:50,559 >> Image processor saved in /kaggle/working/finetuned/checkpoint-24/preprocessor_config.json\n[INFO|tokenization_utils_base.py:2510] 2025-04-01 16:57:50,560 >> tokenizer config file saved in /kaggle/working/finetuned/checkpoint-24/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-04-01 16:57:50,560 >> Special tokens file saved in /kaggle/working/finetuned/checkpoint-24/special_tokens_map.json\n[INFO|processing_utils.py:638] 2025-04-01 16:57:51,324 >> chat template saved in /kaggle/working/finetuned/checkpoint-24/chat_template.json\n[INFO|trainer.py:2665] 2025-04-01 16:57:51,324 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 1530.8095, 'train_samples_per_second': 0.251, 'train_steps_per_second': 0.016, 'train_loss': 0.29538512229919434, 'epoch': 3.0}\n100%|███████████████████████████████████████████| 24/24 [25:29<00:00, 63.73s/it]\n[INFO|image_processing_base.py:261] 2025-04-01 16:57:51,339 >> Image processor saved in /kaggle/working/finetuned/preprocessor_config.json\n[INFO|tokenization_utils_base.py:2510] 2025-04-01 16:57:51,340 >> tokenizer config file saved in /kaggle/working/finetuned/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-04-01 16:57:51,341 >> Special tokens file saved in /kaggle/working/finetuned/special_tokens_map.json\n[INFO|processing_utils.py:638] 2025-04-01 16:57:52,070 >> chat template saved in /kaggle/working/finetuned/chat_template.json\n[INFO|trainer.py:3966] 2025-04-01 16:58:01,836 >> Saving model checkpoint to /kaggle/working/finetuned\n[INFO|configuration_utils.py:699] 2025-04-01 16:58:01,971 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/config.json\n[INFO|configuration_utils.py:771] 2025-04-01 16:58:01,973 >> Model config Qwen2_5_VLConfig {\n  \"architectures\": [\n    \"Qwen2_5_VLForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"image_token_id\": 151655,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 128000,\n  \"max_window_layers\": 70,\n  \"model_type\": \"qwen2_5_vl\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": {\n    \"mrope_section\": [\n      16,\n      24,\n      24\n    ],\n    \"rope_type\": \"default\",\n    \"type\": \"default\"\n  },\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.50.0\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"video_token_id\": 151656,\n  \"vision_config\": {\n    \"depth\": 32,\n    \"fullatt_block_indexes\": [\n      7,\n      15,\n      23,\n      31\n    ],\n    \"hidden_act\": \"silu\",\n    \"hidden_size\": 1280,\n    \"in_channels\": 3,\n    \"in_chans\": 3,\n    \"intermediate_size\": 3420,\n    \"model_type\": \"qwen2_5_vl\",\n    \"num_heads\": 16,\n    \"out_hidden_size\": 2048,\n    \"patch_size\": 14,\n    \"spatial_merge_size\": 2,\n    \"spatial_patch_size\": 14,\n    \"temporal_patch_size\": 2,\n    \"tokens_per_second\": 2,\n    \"window_size\": 112\n  },\n  \"vision_end_token_id\": 151653,\n  \"vision_start_token_id\": 151652,\n  \"vision_token_id\": 151654,\n  \"vocab_size\": 151936\n}\n\n[INFO|tokenization_utils_base.py:2510] 2025-04-01 16:58:02,040 >> tokenizer config file saved in /kaggle/working/finetuned/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-04-01 16:58:02,040 >> Special tokens file saved in /kaggle/working/finetuned/special_tokens_map.json\n***** train metrics *****\n  epoch                    =        3.0\n  total_flos               =     4227GF\n  train_loss               =     0.2954\n  train_runtime            = 0:25:30.80\n  train_samples_per_second =      0.251\n  train_steps_per_second   =      0.016\nFigure saved at: /kaggle/working/finetuned/training_loss.png\n[WARNING|2025-04-01 16:58:03] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n[WARNING|2025-04-01 16:58:03] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n[INFO|modelcard.py:449] 2025-04-01 16:58:03,368 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Merging Fine-tuned Model","metadata":{}},{"cell_type":"markdown","source":"Import packages","metadata":{}},{"cell_type":"code","source":"import json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:03:18.202332Z","iopub.execute_input":"2025-04-01T17:03:18.202860Z","iopub.status.idle":"2025-04-01T17:03:18.207795Z","shell.execute_reply.started":"2025-04-01T17:03:18.202812Z","shell.execute_reply":"2025-04-01T17:03:18.206936Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"Create merging script","metadata":{}},{"cell_type":"code","source":"args = {\n    \"model_name_or_path\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n    \"adapter_name_or_path\": \"/kaggle/working/finetuned\",\n    \"template\": \"qwen2_vl\",\n    \"finetuning_type\": \"lora\",\n    \"trust_remote_code\": True,\n    \"export_dir\": \"/kaggle/working/merged\",\n    \"export_size\": 5,\n    \"export_device\": \"cpu\",\n    \"export_legacy_format\": False,\n}\nwith open(\"merge.json\", \"w\", encoding=\"utf-8\") as f: \n    json.dump(args, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:03:29.133588Z","iopub.execute_input":"2025-04-01T17:03:29.133930Z","iopub.status.idle":"2025-04-01T17:03:29.139177Z","shell.execute_reply.started":"2025-04-01T17:03:29.133899Z","shell.execute_reply":"2025-04-01T17:03:29.138361Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"Merge model","metadata":{}},{"cell_type":"code","source":"!llamafactory-cli export merge.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:03:33.232897Z","iopub.execute_input":"2025-04-01T17:03:33.233201Z","iopub.status.idle":"2025-04-01T17:05:19.728501Z","shell.execute_reply.started":"2025-04-01T17:03:33.233176Z","shell.execute_reply":"2025-04-01T17:05:19.727555Z"}},"outputs":[{"name":"stdout","text":"2025-04-01 17:03:45.071478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-01 17:03:45.221905: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-01 17:03:45.267249: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[2025-04-01 17:03:51,487] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:57,717 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:57,717 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:57,717 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:57,717 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:57,717 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:57,717 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:57,717 >> loading file chat_template.jinja from cache at None\n[INFO|tokenization_utils_base.py:2323] 2025-04-01 17:03:58,097 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[INFO|image_processing_base.py:381] 2025-04-01 17:03:58,245 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n[INFO|image_processing_base.py:381] 2025-04-01 17:03:58,298 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n[WARNING|logging.py:329] 2025-04-01 17:03:58,298 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n[INFO|image_processing_base.py:434] 2025-04-01 17:03:58,301 >> Image processor Qwen2VLImageProcessor {\n  \"do_convert_rgb\": true,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.48145466,\n    0.4578275,\n    0.40821073\n  ],\n  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n  \"image_std\": [\n    0.26862954,\n    0.26130258,\n    0.27577711\n  ],\n  \"max_pixels\": 12845056,\n  \"merge_size\": 2,\n  \"min_pixels\": 3136,\n  \"patch_size\": 14,\n  \"processor_class\": \"Qwen2_5_VLProcessor\",\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"longest_edge\": 12845056,\n    \"shortest_edge\": 3136\n  },\n  \"temporal_patch_size\": 2\n}\n\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:58,354 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:58,354 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:58,354 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:58,354 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:58,354 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:58,354 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-01 17:03:58,355 >> loading file chat_template.jinja from cache at None\n[INFO|tokenization_utils_base.py:2323] 2025-04-01 17:03:58,693 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[INFO|processing_utils.py:876] 2025-04-01 17:03:59,432 >> Processor Qwen2_5_VLProcessor:\n- image_processor: Qwen2VLImageProcessor {\n  \"do_convert_rgb\": true,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.48145466,\n    0.4578275,\n    0.40821073\n  ],\n  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n  \"image_std\": [\n    0.26862954,\n    0.26130258,\n    0.27577711\n  ],\n  \"max_pixels\": 12845056,\n  \"merge_size\": 2,\n  \"min_pixels\": 3136,\n  \"patch_size\": 14,\n  \"processor_class\": \"Qwen2_5_VLProcessor\",\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"longest_edge\": 12845056,\n    \"shortest_edge\": 3136\n  },\n  \"temporal_patch_size\": 2\n}\n\n- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n}\n)\n\n{\n  \"processor_class\": \"Qwen2_5_VLProcessor\"\n}\n\n[INFO|2025-04-01 17:03:59] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n[INFO|configuration_utils.py:699] 2025-04-01 17:03:59,547 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/config.json\n[INFO|configuration_utils.py:771] 2025-04-01 17:03:59,549 >> Model config Qwen2_5_VLConfig {\n  \"architectures\": [\n    \"Qwen2_5_VLForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"image_token_id\": 151655,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 128000,\n  \"max_window_layers\": 70,\n  \"model_type\": \"qwen2_5_vl\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": {\n    \"mrope_section\": [\n      16,\n      24,\n      24\n    ],\n    \"rope_type\": \"default\",\n    \"type\": \"default\"\n  },\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.50.0\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"video_token_id\": 151656,\n  \"vision_config\": {\n    \"depth\": 32,\n    \"fullatt_block_indexes\": [\n      7,\n      15,\n      23,\n      31\n    ],\n    \"hidden_act\": \"silu\",\n    \"hidden_size\": 1280,\n    \"in_channels\": 3,\n    \"in_chans\": 3,\n    \"intermediate_size\": 3420,\n    \"model_type\": \"qwen2_5_vl\",\n    \"num_heads\": 16,\n    \"out_hidden_size\": 2048,\n    \"patch_size\": 14,\n    \"spatial_merge_size\": 2,\n    \"spatial_patch_size\": 14,\n    \"temporal_patch_size\": 2,\n    \"tokens_per_second\": 2,\n    \"window_size\": 112\n  },\n  \"vision_end_token_id\": 151653,\n  \"vision_start_token_id\": 151652,\n  \"vision_token_id\": 151654,\n  \"vocab_size\": 151936\n}\n\n[INFO|2025-04-01 17:03:59] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n[INFO|modeling_utils.py:1154] 2025-04-01 17:03:59,607 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/model.safetensors.index.json\n[INFO|modeling_utils.py:2170] 2025-04-01 17:03:59,614 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.\n[INFO|configuration_utils.py:1139] 2025-04-01 17:03:59,616 >> Generate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\n[INFO|modeling_utils.py:2170] 2025-04-01 17:03:59,616 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.29it/s]\n[INFO|modeling_utils.py:4987] 2025-04-01 17:04:01,694 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.\n\n[INFO|modeling_utils.py:4995] 2025-04-01 17:04:01,694 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-3B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1094] 2025-04-01 17:04:01,756 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/generation_config.json\n[INFO|configuration_utils.py:1139] 2025-04-01 17:04:01,756 >> Generate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.05,\n  \"temperature\": 1e-06\n}\n\n[INFO|2025-04-01 17:04:01] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n[INFO|2025-04-01 17:04:42] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n[INFO|2025-04-01 17:04:42] llamafactory.model.adapter:143 >> Loaded adapter(s): /kaggle/working/finetuned\n[INFO|2025-04-01 17:04:42] llamafactory.model.loader:143 >> all params: 3,754,622,976\n[INFO|2025-04-01 17:04:42] llamafactory.train.tuner:143 >> Convert model dtype to: torch.bfloat16.\n[INFO|configuration_utils.py:423] 2025-04-01 17:04:42,070 >> Configuration saved in /kaggle/working/merged/config.json\n[INFO|configuration_utils.py:908] 2025-04-01 17:04:42,071 >> Configuration saved in /kaggle/working/merged/generation_config.json\n[INFO|modeling_utils.py:3594] 2025-04-01 17:05:15,281 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /kaggle/working/merged/model.safetensors.index.json.\n[INFO|tokenization_utils_base.py:2510] 2025-04-01 17:05:15,282 >> tokenizer config file saved in /kaggle/working/merged/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-04-01 17:05:15,283 >> Special tokens file saved in /kaggle/working/merged/special_tokens_map.json\n[INFO|image_processing_base.py:261] 2025-04-01 17:05:15,524 >> Image processor saved in /kaggle/working/merged/preprocessor_config.json\n[INFO|tokenization_utils_base.py:2510] 2025-04-01 17:05:15,552 >> tokenizer config file saved in /kaggle/working/merged/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-04-01 17:05:15,553 >> Special tokens file saved in /kaggle/working/merged/special_tokens_map.json\n[INFO|processing_utils.py:638] 2025-04-01 17:05:16,491 >> chat template saved in /kaggle/working/merged/chat_template.json\n[INFO|2025-04-01 17:05:16] llamafactory.train.tuner:143 >> Ollama modelfile saved in /kaggle/working/merged/Modelfile\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"Zip the output","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!7z a -r finetuned.zip finetuned\n!7z a -r merged.zip merged","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:20:38.025537Z","iopub.execute_input":"2025-04-01T17:20:38.025880Z","iopub.status.idle":"2025-04-01T17:28:29.271635Z","shell.execute_reply.started":"2025-04-01T17:20:38.025854Z","shell.execute_reply":"2025-04-01T17:28:29.270719Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs Intel(R) Xeon(R) CPU @ 2.00GHz (50653),ASM,AES-NI)\n\nOpen archive: finetuned.zip\n--\nPath = finetuned.zip\nType = zip\nPhysical Size = 221819401\n\nScanning the drive:\n  0M Sca        5 folders, 40 files, 272561333 bytes (260 MiB)\n\nUpdating archive: finetuned.zip\n\nItems to compress: 45\n\n      3% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                              6% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             10% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             13% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             17% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             20% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             24% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             28% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             31% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             34% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             37% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             41% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             45% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             48% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             51% 13 U finetuned/checkpoint-24/global_s . rank_1_mp_rank_00_optim_states.                                                                             54% 17 U finetuned/checkpoint-24/merges.t                                           57% 23 U finetuned/checkpoint-24/tokenizer.js                                               60% 27 U finetuned/checkpoint-24/vocab.js                                           63% 27 U finetuned/checkpoint-24/vocab.js                                           65% 27 U finetuned/checkpoint-24/vocab.js                                           68% 29 U finetuned/merges.t                             71% 29 U finetuned/merges.t                             73% 33 U finetuned/tokenizer.js                                 76% 33 U finetuned/tokenizer.js                                 78% 33 U finetuned/tokenizer.js                                 80% 33 U finetuned/tokenizer.js                                 83% 33 U finetuned/tokenizer.js                                 86% 33 U finetuned/tokenizer.js                                 88% 33 U finetuned/tokenizer.js                                 91% 33 U finetuned/tokenizer.js                                 94% 33 U finetuned/tokenizer.js                                 96% 40 U finetuned/vocab.js                             98% 40 U finetuned/vocab.js                             99% 40 U finetuned/vocab.js                             99% 40 = kaggle/working/finetun                                \nFiles read from disk: 40\nArchive size: 219934376 bytes (210 MiB)\nEverything is Ok\n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs Intel(R) Xeon(R) CPU @ 2.00GHz (50653),ASM,AES-NI)\n\nOpen archive: merged.zip\n--\nPath = merged.zip\nType = zip\nPhysical Size = 336\n\nScanning the drive:\n  0M Sca        1 folder, 14 files, 7525286269 bytes (7177 MiB)\n\nUpdating archive: merged.zip\n\nItems to compress: 15\n\n      0% 12 + merged/tokenizer.jso                                0% 12 + merged/tokenizer_config.js                                      0% 14 + merged/vocab.jso                            1% 14 + merged/vocab.jso                            2% 14 + merged/vocab.jso                            3% 14 + merged/vocab.jso                            4% 14 + merged/vocab.jso                            5% 14 + merged/vocab.jso                            6% 14 + merged/vocab.jso                            7% 14 + merged/vocab.jso                            8% 14 + merged/vocab.jso                            9% 14 + merged/vocab.jso                           10% 14 + merged/vocab.jso                           11% 14 + merged/vocab.jso                           12% 14 + merged/vocab.jso                           13% 14 + merged/vocab.jso                           14% 14 + merged/vocab.jso                           15% 14 + merged/vocab.jso                           16% 14 + merged/vocab.jso                           17% 14 + merged/vocab.jso                           18% 14 + merged/vocab.jso                           19% 14 + merged/vocab.jso                           20% 14 + merged/vocab.jso                           21% 14 + merged/vocab.jso                           22% 14 + merged/vocab.jso                           23% 14 + merged/vocab.jso                           24% 14 + merged/vocab.jso                           25% 14 + merged/vocab.jso                           26% 14 + merged/vocab.jso                           27% 14 + merged/vocab.jso                           28% 14 + merged/vocab.jso                           29% 14 + merged/vocab.jso                           30% 14 + merged/vocab.jso                           31% 14 + merged/vocab.jso                           32% 14 + merged/vocab.jso                           33% 14 + merged/vocab.jso                           34% 14 + merged/vocab.jso                           35% 14 + merged/vocab.jso                           36% 14 + merged/vocab.jso                           37% 14 + merged/vocab.jso                           38% 14 + merged/vocab.jso                           39% 14 + merged/vocab.jso                           40% 14 + merged/vocab.jso                           41% 14 + merged/vocab.jso                           42% 14 + merged/vocab.jso                           43% 14 + merged/vocab.jso                           44% 14 + merged/vocab.jso                           45% 14 + merged/vocab.jso                           46% 14 + merged/vocab.jso                           47% 14 + merged/vocab.jso                           48% 14 + merged/vocab.jso                           49% 14 + merged/vocab.jso                           50% 14 + merged/vocab.jso                           51% 14 + merged/vocab.jso                           52% 14 + merged/vocab.jso                           53% 14 + merged/vocab.jso                           54% 14 + merged/vocab.jso                           55% 14 + merged/vocab.jso                           56% 14 + merged/vocab.jso                           57% 14 + merged/vocab.jso                           58% 14 + merged/vocab.jso                           59% 14 + merged/vocab.jso                           60% 14 + merged/vocab.jso                           61% 14 + merged/vocab.jso                           62% 14 + merged/vocab.jso                           63% 14 + merged/vocab.jso                           64% 14 + merged/vocab.jso                           65% 14 + merged/vocab.jso                           66% 14 + merged/vocab.jso                           67% 14 + merged/vocab.jso                           68% 14 + merged/vocab.jso                           69% 14 + merged/vocab.jso                           70% 14 + merged/vocab.jso                           71% 14 + merged/vocab.jso                           72% 14 + merged/vocab.jso                           73% 14 + merged/vocab.jso                           74% 14 + merged/vocab.jso                           75% 14 + merged/vocab.jso                           76% 14 + merged/vocab.jso                           77% 14 + merged/vocab.jso                           78% 14 + merged/vocab.jso                           79% 14 + merged/vocab.jso                           80% 14 + merged/vocab.jso                           81% 14 + merged/vocab.jso                           82% 14 + merged/vocab.jso                           83% 14 + merged/vocab.jso                           84% 14 + merged/vocab.jso                           85% 14 + merged/vocab.jso                           86% 14 + merged/vocab.jso                           87% 14 + merged/vocab.jso                           88% 14 + merged/vocab.jso                           89% 14 + merged/vocab.jso                           90% 14 + merged/vocab.jso                           91% 14 + merged/vocab.jso                           92% 14 + merged/vocab.jso                           93% 14 + merged/vocab.jso                           94% 14 + merged/vocab.jso                           95% 14 + merged/vocab.jso                           96% 14 + merged/vocab.jso                           97% 14 + merged/vocab.jso                           98% 14 + merged/vocab.jso                           99% 14 + merged/vocab.jso                          \nFiles read from disk: 14\nArchive size: 5955952152 bytes (5681 MiB)\nEverything is Ok\n","output_type":"stream"}],"execution_count":30}]}