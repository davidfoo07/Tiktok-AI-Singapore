{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11193225,"sourceType":"datasetVersion","datasetId":6987776}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"18ab9fe8abd84251a729fefe3bfa0ad9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23d09e6258784367bee4067b800fceb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48d7994725b24a28ac9c79418c5111a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7417c346afc24a98832d481d58167b69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae6c27aa30f34ddeba9abd06bf96cafb","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93bdf8798f5f42718bb89b101e54c682","value":2}},"7d5805c372ee40cdb860c69d0d87c87b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8e66c28e3e54a3ea111ef70b229d8a9","placeholder":"​","style":"IPY_MODEL_23d09e6258784367bee4067b800fceb9","value":"Loading checkpoint shards: 100%"}},"93bdf8798f5f42718bb89b101e54c682":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae6c27aa30f34ddeba9abd06bf96cafb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b11f7e2e0acc40128f1ffc5f36b5b029":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d5805c372ee40cdb860c69d0d87c87b","IPY_MODEL_7417c346afc24a98832d481d58167b69","IPY_MODEL_f73f2c3f4e1347dcb0ff085a27d1927a"],"layout":"IPY_MODEL_48d7994725b24a28ac9c79418c5111a6"}},"b5dbaea6b0ce4330921b1486f1c4f7d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8e66c28e3e54a3ea111ef70b229d8a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f73f2c3f4e1347dcb0ff085a27d1927a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18ab9fe8abd84251a729fefe3bfa0ad9","placeholder":"​","style":"IPY_MODEL_b5dbaea6b0ce4330921b1486f1c4f7d8","value":" 2/2 [00:34&lt;00:00, 17.13s/it]"}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting up environment","metadata":{}},{"cell_type":"markdown","source":"Check cuda version","metadata":{"id":"NdvT46W7JVp-"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"WsGKOY7CJO_p","outputId":"faf19e6b-cd94-427a-c135-534112e1cf10","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:04:57.465946Z","iopub.execute_input":"2025-03-28T09:04:57.466253Z","iopub.status.idle":"2025-03-28T09:04:57.687007Z","shell.execute_reply.started":"2025-03-28T09:04:57.466225Z","shell.execute_reply":"2025-03-28T09:04:57.686035Z"}},"outputs":[{"name":"stdout","text":"Fri Mar 28 09:04:57 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   45C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   45C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Change CUDA memory config","metadata":{"id":"o68lGIFRPq2a"}},{"cell_type":"code","source":"!export 'PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True'","metadata":{"id":"Jg-aQ_mEPhcr","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:04:57.688406Z","iopub.execute_input":"2025-03-28T09:04:57.688711Z","iopub.status.idle":"2025-03-28T09:04:57.803014Z","shell.execute_reply.started":"2025-03-28T09:04:57.688687Z","shell.execute_reply":"2025-03-28T09:04:57.802000Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Install packages","metadata":{"id":"U0zIj5U8IYpW"}},{"cell_type":"code","source":"%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n%pip install --upgrade autoawq\n%pip install --upgrade datasets transformers trl peft wandb\n%pip install --upgrade qwen-vl-utils[decord]","metadata":{"id":"qxeL2VoxJDIj","outputId":"8a8366e4-eb9a-47f3-b3ef-96e2f94e6224","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:04:57.804812Z","iopub.execute_input":"2025-03-28T09:04:57.805026Z","iopub.status.idle":"2025-03-28T09:05:43.168505Z","shell.execute_reply.started":"2025-03-28T09:04:57.805007Z","shell.execute_reply":"2025-03-28T09:05:43.167536Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu126\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\nCollecting autoawq\n  Downloading autoawq-0.2.8.tar.gz (71 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from autoawq) (2.5.1+cu121)\nCollecting triton (from autoawq)\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: transformers<=4.47.1,>=4.45.0 in /usr/local/lib/python3.10/dist-packages (from autoawq) (4.47.0)\nRequirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from autoawq) (0.21.0)\nRequirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from autoawq) (4.12.2)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from autoawq) (1.2.1)\nRequirement already satisfied: datasets>=2.20 in /usr/local/lib/python3.10/dist-packages (from autoawq) (3.3.1)\nCollecting zstandard (from autoawq)\n  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: huggingface_hub>=0.26.5 in /usr/local/lib/python3.10/dist-packages (from autoawq) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.20->autoawq) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (3.11.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20->autoawq) (6.0.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.1->autoawq) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.1->autoawq) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.1->autoawq) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.5.1->autoawq) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.47.1,>=4.45.0->autoawq) (2024.11.6)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.47.1,>=4.45.0->autoawq) (0.4.5)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->autoawq) (5.9.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20->autoawq) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20->autoawq) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20->autoawq) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20->autoawq) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20->autoawq) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets>=2.20->autoawq) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets>=2.20->autoawq) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets>=2.20->autoawq) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets>=2.20->autoawq) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets>=2.20->autoawq) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets>=2.20->autoawq) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.5.1->autoawq) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.20->autoawq) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.20->autoawq) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.20->autoawq) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets>=2.20->autoawq) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets>=2.20->autoawq) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets>=2.20->autoawq) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets>=2.20->autoawq) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets>=2.20->autoawq) (2024.2.0)\nDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: autoawq\n  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for autoawq: filename=autoawq-0.2.8-py3-none-any.whl size=108744 sha256=fc9978454fe6a5df242c2e1046c183f02370dafaacc33b82448536444b0c83a2\n  Stored in directory: /root/.cache/pip/wheels/5c/09/7e/7154d95be3598ce30655fa68ff29097a307324e0430625e3c6\nSuccessfully built autoawq\nInstalling collected packages: triton, zstandard, autoawq\nSuccessfully installed autoawq-0.2.8 triton-3.2.0 zstandard-0.23.0\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting datasets\n  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting transformers\n  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\nCollecting trl\n  Downloading trl-0.16.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting peft\n  Downloading peft-0.15.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.2.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.50.2-py3-none-any.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading trl-0.16.0-py3-none-any.whl (335 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.15.1-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: transformers, datasets, trl, peft\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.3.1\n    Uninstalling datasets-3.3.1:\n      Successfully uninstalled datasets-3.3.1\n  Attempting uninstall: peft\n    Found existing installation: peft 0.14.0\n    Uninstalling peft-0.14.0:\n      Successfully uninstalled peft-0.14.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nautoawq 0.2.8 requires transformers<=4.47.1,>=4.45.0, but you have transformers 4.50.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-3.5.0 peft-0.15.1 transformers-4.50.2 trl-0.16.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting qwen-vl-utils[decord]\n  Downloading qwen_vl_utils-0.0.10-py3-none-any.whl.metadata (6.3 kB)\nCollecting av (from qwen-vl-utils[decord])\n  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils[decord]) (24.2)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils[decord]) (11.0.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils[decord]) (2.32.3)\nCollecting decord (from qwen-vl-utils[decord])\n  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord->qwen-vl-utils[decord]) (1.26.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils[decord]) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils[decord]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils[decord]) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils[decord]) (2025.1.31)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord->qwen-vl-utils[decord]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord->qwen-vl-utils[decord]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord->qwen-vl-utils[decord]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord->qwen-vl-utils[decord]) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord->qwen-vl-utils[decord]) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord->qwen-vl-utils[decord]) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord->qwen-vl-utils[decord]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord->qwen-vl-utils[decord]) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->decord->qwen-vl-utils[decord]) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->decord->qwen-vl-utils[decord]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->decord->qwen-vl-utils[decord]) (2024.2.0)\nDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading qwen_vl_utils-0.0.10-py3-none-any.whl (6.7 kB)\nInstalling collected packages: av, qwen-vl-utils, decord\nSuccessfully installed av-14.2.0 decord-0.6.0 qwen-vl-utils-0.0.10\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Login to wandb","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb\n\nuser_secrets = UserSecretsClient()\n\nmy_secret = user_secrets.get_secret(\"WANDB_API_KEY\") \n\nwandb.login(key=my_secret)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:14:12.683133Z","iopub.execute_input":"2025-03-28T09:14:12.683453Z","iopub.status.idle":"2025-03-28T09:14:21.169010Z","shell.execute_reply.started":"2025-03-28T09:14:12.683432Z","shell.execute_reply":"2025-03-28T09:14:21.168315Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseanjeanmoey123\u001b[0m (\u001b[33mseanjeanmoey123-nus\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"markdown","source":"Import packages","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets, Value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:13.311668Z","iopub.execute_input":"2025-03-28T09:21:13.311994Z","iopub.status.idle":"2025-03-28T09:21:16.226997Z","shell.execute_reply.started":"2025-03-28T09:21:13.311965Z","shell.execute_reply":"2025-03-28T09:21:16.226075Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"System message","metadata":{}},{"cell_type":"code","source":"system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from videos.\nYour task is to analyze the provided video and respond to queries with concise answers, usually a single letter, or a few sentences.\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:19.846580Z","iopub.execute_input":"2025-03-28T09:21:19.847041Z","iopub.status.idle":"2025-03-28T09:21:19.852448Z","shell.execute_reply.started":"2025-03-28T09:21:19.846987Z","shell.execute_reply":"2025-03-28T09:21:19.851405Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Format data (TODO)","metadata":{}},{"cell_type":"code","source":"def format_data(sample):\n    return [\n        {\n            \"role\": \"system\",\n            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"video\",\n                    \"video\": f\"../input/next-qa-dataset/NExTVideo/{sample['video']}.mp4\",\n                    \"max_pixels\": 135 * 240,\n                    \"fps\": 1,\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": sample[\"question\"],\n                },\n            ],\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [{\"type\": \"text\", \"text\": sample[\"answer\"]}],\n        },\n    ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:21.346258Z","iopub.execute_input":"2025-03-28T09:21:21.346577Z","iopub.status.idle":"2025-03-28T09:21:21.351052Z","shell.execute_reply.started":"2025-03-28T09:21:21.346549Z","shell.execute_reply":"2025-03-28T09:21:21.350271Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Formatting MCQ","metadata":{}},{"cell_type":"code","source":"def reformat_mcq(sample):\n    choice_labels = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n    choices = [sample[f\"a{i}\"] for i in range(5)]\n    formatted_choices = \"\\n\".join([f\"{choice_labels[i]}. {choice}\" for i, choice in enumerate(choices)])\n    \n    return {\n        \"video\": sample[\"video\"],\n        \"frame_count\": sample[\"frame_count\"],\n        \"width\": sample[\"width\"],\n        \"height\": sample[\"height\"],\n        \"question\": f\"{sample['question']}\\n{formatted_choices}\\nSelect one best answer to the above multiple-choice question based on the video. Respond with only the letter (A, B, C, D or E) of the correct option.\",\n        \"answer\": choice_labels[sample[\"answer\"]],\n        \"qid\": sample[\"qid\"],\n        \"type\": sample[\"type\"],\n        \"additional_ref_answer\": None\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:23.381721Z","iopub.execute_input":"2025-03-28T09:21:23.382008Z","iopub.status.idle":"2025-03-28T09:21:23.387065Z","shell.execute_reply.started":"2025-03-28T09:21:23.381985Z","shell.execute_reply":"2025-03-28T09:21:23.386195Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Load dataset","metadata":{}},{"cell_type":"code","source":"dataset_id = 'lmms-lab/NExTQA'\n\nmcq_dataset = load_dataset(dataset_id, 'MC')['test'].map(reformat_mcq, remove_columns=['a0', 'a1', 'a2', 'a3', 'a4'])\nnew_features = mcq_dataset.features.copy()\nnew_features[\"video\"] = Value(\"string\")\nnew_features[\"frame_count\"] = Value(\"int32\")\nnew_features[\"width\"] = Value(\"int32\")\nnew_features[\"height\"] = Value(\"int32\")\nnew_features[\"qid\"] = Value(\"int32\")\nmcq_dataset = mcq_dataset.cast(new_features)\ntrain_test_split = mcq_dataset.train_test_split(test_size=0.3, seed=42)\nval_test_split = train_test_split['test'].train_test_split(test_size=2/3, seed=42)\nmcq_train_dataset = train_test_split['train']\nmcq_eval_dataset = val_test_split['train']\nmcq_test_dataset = val_test_split['test']\n\noe_train_dataset, oe_eval_dataset, oe_test_dataset = load_dataset(dataset_id, 'OE', split=['train', 'validation', 'test'])\n\ntrain_dataset = concatenate_datasets([mcq_train_dataset, oe_train_dataset])\neval_dataset = concatenate_datasets([mcq_eval_dataset, oe_eval_dataset])\ntest_dataset = concatenate_datasets([mcq_test_dataset, oe_test_dataset])\n\ntrain_dataset = [format_data(sample) for sample in train_dataset]\neval_dataset = [format_data(sample) for sample in eval_dataset]\ntest_dataset = [format_data(sample) for sample in test_dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:30.950145Z","iopub.execute_input":"2025-03-28T09:21:30.950479Z","iopub.status.idle":"2025-03-28T09:21:49.850214Z","shell.execute_reply.started":"2025-03-28T09:21:30.950451Z","shell.execute_reply":"2025-03-28T09:21:49.849468Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24abef11cd214528954c61acc912ea5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f14eb9161a824c28abb74aeaf942ed15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/8564 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46bc7c38664541ae9ce8bdc8870afcc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8564 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3e576d17a894b5e95f536087812b5c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"332e78447045445893d0767cbfb2a812"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"798c0e8aa8e34c89a8cb0b627a660d4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/289k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db05fcbe04ae4cacb84f9ba134a0e67f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/572k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db8edf389f0f4f138fb191bf0fc3e3ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/37523 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d1727b475e0484295a85a51130768eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/5343 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0070ca9d3f41e09c509b92752b6b78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/9178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c21d25d50b52431eaca47e6fcb5178dc"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['video', 'frame_count', 'width', 'height', 'question', 'answer', 'qid', 'type', 'additional_ref_answer'],\n    num_rows: 43517\n})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/43517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72b8a3d74c7f4ed3bf03dea58cd7693e"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['video', 'frame_count', 'width', 'height', 'question', 'answer', 'qid', 'type', 'additional_ref_answer'],\n    num_rows: 5994\n})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Loading Model","metadata":{}},{"cell_type":"markdown","source":"Import packages","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:05:55.878180Z","iopub.execute_input":"2025-03-28T09:05:55.878437Z","iopub.status.idle":"2025-03-28T09:06:12.621413Z","shell.execute_reply.started":"2025-03-28T09:05:55.878417Z","shell.execute_reply":"2025-03-28T09:06:12.620726Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Load model","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct-AWQ\",\n    torch_dtype=torch.float16,\n    device_map=device\n)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct-AWQ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:06:12.622132Z","iopub.execute_input":"2025-03-28T09:06:12.622670Z","iopub.status.idle":"2025-03-28T09:06:39.083571Z","shell.execute_reply.started":"2025-03-28T09:06:12.622645Z","shell.execute_reply":"2025-03-28T09:06:39.082607Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba8a6bf2976646b598952c3c5afd4b8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/89.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"795f8091c18e46a6ab06c9d9e1a428d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c9f229f677b466eaf0d7ec0504e2817"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e360d733eb440e89eca2effc8a1baa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c7a4d95d16b47a791d1520bd183511a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4702f64e8bb8425dbbc9a9569e57c539"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct-AWQ were not used when initializing Qwen2_5_VLForConditionalGeneration: ['lm_head.weight']\n- This IS expected if you are initializing Qwen2_5_VLForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Qwen2_5_VLForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Qwen2_5_VLForConditionalGeneration were not initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct-AWQ and are newly initialized: ['lm_head.qweight', 'lm_head.qzeros', 'lm_head.scales']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/249 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb602e54533d4fedab60e3e7e0af9447"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/575 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba232b658bcb4c15b962d287eb608fd3"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1a1571eec4942bfafa740e06c9c090b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84ba7d7c68df495f89629b8793147c6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"753a5f9991674f488e38e52323d99d64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0defd8027ee4182bd0c5375a335de82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1225a3bb3c04eb5bc77c9659f46593b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54f5384fce60475b89e17ecb8227f898"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62cb23d5107c4f6b959721ea37b24716"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Fine-tuning Model","metadata":{}},{"cell_type":"markdown","source":"Import packages","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom trl import SFTConfig, SFTTrainer\nimport wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:09:49.179668Z","iopub.execute_input":"2025-03-28T09:09:49.180035Z","iopub.status.idle":"2025-03-28T09:09:49.185110Z","shell.execute_reply.started":"2025-03-28T09:09:49.180000Z","shell.execute_reply":"2025-03-28T09:09:49.184049Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"Set up QLoRA","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.05,\n    r=8,\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"v_proj\"],\n    task_type=\"CAUSAL_LM\",\n)\npeft_model = get_peft_model(model, peft_config)\npeft_model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:06:49.992104Z","iopub.execute_input":"2025-03-28T09:06:49.992347Z","iopub.status.idle":"2025-03-28T09:06:50.147449Z","shell.execute_reply.started":"2025-03-28T09:06:49.992326Z","shell.execute_reply":"2025-03-28T09:06:50.146812Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2,523,136 || all params: 1,224,274,944 || trainable%: 0.2061\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"Set up SFTConfig","metadata":{}},{"cell_type":"code","source":"training_args = SFTConfig(\n    output_dir=\"qwen2.5-7b-instruct-trl-sft-NextQA\",  # Directory to save the model\n    num_train_epochs=3,  # Number of training epochs\n    per_device_train_batch_size=4,  # Batch size for training\n    per_device_eval_batch_size=4,  # Batch size for evaluation\n    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n    # Optimizer and scheduler settings\n    optim=\"adamw_torch_fused\",  # Optimizer type\n    learning_rate=2e-4,  # Learning rate for training\n    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n    # Logging and evaluation\n    logging_steps=10,  # Steps interval for logging\n    eval_steps=10,  # Steps interval for evaluation\n    eval_strategy=\"steps\",  # Strategy for evaluation\n    save_strategy=\"steps\",  # Strategy for saving the model\n    save_steps=20,  # Steps interval for saving\n    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n    greater_is_better=False,  # Whether higher metric values are better\n    load_best_model_at_end=True,  # Load the best model after training\n    # Mixed precision and gradient settings\n    bf16=True,  # Use bfloat16 precision\n    tf32=False,  # Use TensorFloat-32 precision\n    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n    warmup_ratio=0.03,  # Ratio of total steps for warmup\n    # Hub\n    push_to_hub=False,  # Whether to push model to Hugging Face Hub\n    report_to=\"wandb\",  # Reporting tool for tracking metrics\n    # Gradient checkpointing settings\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n    # Dataset configuration\n    dataset_text_field=\"\",  # Text field in dataset\n    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n    # max_seq_length=1024  # Maximum sequence length for input\n)\ntraining_args.remove_unused_columns = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:06:50.148119Z","iopub.execute_input":"2025-03-28T09:06:50.148336Z","iopub.status.idle":"2025-03-28T09:06:50.183530Z","shell.execute_reply.started":"2025-03-28T09:06:50.148316Z","shell.execute_reply":"2025-03-28T09:06:50.182909Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Set up wandb","metadata":{}},{"cell_type":"code","source":"wandb.init(\n    project=\"qwen2.5-7b-instruct-trl-sft-NextQA\",\n    name=\"qwen2.5-7b-instruct-trl-sft-NextQA\",\n    config=training_args,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:09:53.623692Z","iopub.execute_input":"2025-03-28T09:09:53.624037Z","iopub.status.idle":"2025-03-28T09:09:59.919751Z","shell.execute_reply.started":"2025-03-28T09:09:53.624005Z","shell.execute_reply":"2025-03-28T09:09:59.919038Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseanjeanmoey123\u001b[0m (\u001b[33mseanjeanmoey123-nus\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250328_090953-cmzbx6qz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/seanjeanmoey123-nus/qwen2.5-7b-instruct-trl-sft-NextQA/runs/cmzbx6qz' target=\"_blank\">qwen2.5-7b-instruct-trl-sft-NextQA</a></strong> to <a href='https://wandb.ai/seanjeanmoey123-nus/qwen2.5-7b-instruct-trl-sft-NextQA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/seanjeanmoey123-nus/qwen2.5-7b-instruct-trl-sft-NextQA' target=\"_blank\">https://wandb.ai/seanjeanmoey123-nus/qwen2.5-7b-instruct-trl-sft-NextQA</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/seanjeanmoey123-nus/qwen2.5-7b-instruct-trl-sft-NextQA/runs/cmzbx6qz' target=\"_blank\">https://wandb.ai/seanjeanmoey123-nus/qwen2.5-7b-instruct-trl-sft-NextQA/runs/cmzbx6qz</a>"},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/seanjeanmoey123-nus/qwen2.5-7b-instruct-trl-sft-NextQA/runs/cmzbx6qz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7930242599c0>"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"Collator function","metadata":{}},{"cell_type":"code","source":"def collate_fn(examples):\n    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]\n    \n    image_inputs = [process_vision_info(example)[0] for example in examples]\n    video_inputs = [process_vision_info(example)[1] for example in examples]\n    \n    batch = processor(\n        text=texts, images=image_inputs, videos=video_inputs, return_tensors=\"pt\", padding=True,\n    )\n\n    labels = batch[\"input_ids\"].clone()\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    \n    image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n    for image_token_id in image_tokens:\n        labels[labels == image_token_id] = -100\n        \n    batch[\"labels\"] = labels\n\n    return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:06:50.184113Z","iopub.execute_input":"2025-03-28T09:06:50.184342Z","iopub.status.idle":"2025-03-28T09:06:50.189715Z","shell.execute_reply.started":"2025-03-28T09:06:50.184310Z","shell.execute_reply":"2025-03-28T09:06:50.188947Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"Set up trainer","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=collate_fn,\n    peft_config=peft_config,\n    processing_class=processor.tokenizer,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:06:50.190512Z","iopub.execute_input":"2025-03-28T09:06:50.190768Z","iopub.status.idle":"2025-03-28T09:06:50.454558Z","shell.execute_reply.started":"2025-03-28T09:06:50.190747Z","shell.execute_reply":"2025-03-28T09:06:50.453879Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Train model","metadata":{}},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model(training_args.output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:10:03.741319Z","iopub.execute_input":"2025-03-28T09:10:03.741645Z","iopub.status.idle":"2025-03-28T09:10:04.461513Z","shell.execute_reply.started":"2025-03-28T09:10:03.741617Z","shell.execute_reply":"2025-03-28T09:10:04.460206Z"}},"outputs":[{"name":"stderr","text":"qwen-vl-utils using decord to read video.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/qwen_vl_utils/vision_process.py\u001b[0m in \u001b[0;36mfetch_video\u001b[0;34m(ele, image_factor, return_video_sample_fps)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVIDEO_READER_BACKENDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_reader_backend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/qwen_vl_utils/vision_process.py\u001b[0m in \u001b[0;36m_read_video_decord\u001b[0;34m(ele)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mvr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;31m# TODO: support start_pts and end_pts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/decord/video_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, ctx, width, height, num_threads, fault_tol)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error reading \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muri\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CAPI_VideoReaderGetFrameCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error reading ../input/next-qa-dataset/NExTVideo/8040040139.mp4...","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/io/video.py\u001b[0m in \u001b[0;36mread_video\u001b[0;34m(filename, start_pts, end_pts, pts_unit, output_format)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/av/container/core.pyx\u001b[0m in \u001b[0;36mav.container.core.open\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/av/container/core.pyx\u001b[0m in \u001b[0;36mav.container.core.Container.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/av/container/core.pyx\u001b[0m in \u001b[0;36mav.container.core.Container.err_check\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/av/error.pyx\u001b[0m in \u001b[0;36mav.error.err_check\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/next-qa-dataset/NExTVideo/8040040139.mp4'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-8f424b778364>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2506\u001b[0m                 \u001b[0mupdate_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m                 \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mupdate_step\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_updates\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mremainder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m                     \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[1;32m   5222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5223\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5224\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5225\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5226\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-7406bf0a8f3c>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimage_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocess_vision_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvideo_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocess_vision_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-7406bf0a8f3c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimage_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocess_vision_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvideo_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocess_vision_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/qwen_vl_utils/vision_process.py\u001b[0m in \u001b[0;36mprocess_vision_info\u001b[0;34m(conversations, return_video_kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mimage_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvision_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"video\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvision_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mvideo_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_sample_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvision_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_video_sample_fps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0mvideo_sample_fps_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_sample_fps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mvideo_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/qwen_vl_utils/vision_process.py\u001b[0m in \u001b[0;36mfetch_video\u001b[0;34m(ele, image_factor, return_video_sample_fps)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"video_reader_backend {video_reader_backend} error, use torchvision as default, msg: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVIDEO_READER_BACKENDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"torchvision\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mnframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/qwen_vl_utils/vision_process.py\u001b[0m in \u001b[0;36m_read_video_torchvision\u001b[0;34m(ele)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     video, audio, info = io.read_video(\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mstart_pts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"video_start\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/io/video.py\u001b[0m in \u001b[0;36mread_video\u001b[0;34m(filename, start_pts, end_pts, pts_unit, output_format)\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audio_fps\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAVError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0;31m# TODO raise a warning?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'av' has no attribute 'AVError'"],"ename":"AttributeError","evalue":"module 'av' has no attribute 'AVError'","output_type":"error"}],"execution_count":23}]}