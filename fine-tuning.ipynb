{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdvT46W7JVp-"
   },
   "source": [
    "Check cuda version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T08:54:04.433342Z",
     "iopub.status.busy": "2025-04-01T08:54:04.433012Z",
     "iopub.status.idle": "2025-04-01T08:54:04.652202Z",
     "shell.execute_reply": "2025-04-01T08:54:04.651377Z",
     "shell.execute_reply.started": "2025-04-01T08:54:04.433319Z"
    },
    "id": "WsGKOY7CJO_p",
    "outputId": "faf19e6b-cd94-427a-c135-534112e1cf10",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  1 08:54:04 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   46C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   49C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:48:40.727865Z",
     "iopub.status.busy": "2025-04-01T12:48:40.727492Z",
     "iopub.status.idle": "2025-04-01T12:48:42.112380Z",
     "shell.execute_reply": "2025-04-01T12:48:42.111573Z",
     "shell.execute_reply.started": "2025-04-01T12:48:40.727835Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 346, done.\u001b[K\n",
      "remote: Counting objects: 100% (346/346), done.\u001b[K\n",
      "remote: Compressing objects: 100% (285/285), done.\u001b[K\n",
      "remote: Total 346 (delta 80), reused 168 (delta 45), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (346/346), 9.54 MiB | 32.01 MiB/s, done.\n",
      "Resolving deltas: 100% (80/80), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:48:44.746588Z",
     "iopub.status.busy": "2025-04-01T12:48:44.746301Z",
     "iopub.status.idle": "2025-04-01T12:48:44.753403Z",
     "shell.execute_reply": "2025-04-01T12:48:44.752652Z",
     "shell.execute_reply.started": "2025-04-01T12:48:44.746567Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/LLaMA-Factory\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0zIj5U8IYpW"
   },
   "source": [
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:49:16.046017Z",
     "iopub.status.busy": "2025-04-01T12:49:16.045702Z",
     "iopub.status.idle": "2025-04-01T12:50:30.678466Z",
     "shell.execute_reply": "2025-04-01T12:50:30.677230Z",
     "shell.execute_reply.started": "2025-04-01T12:49:16.045993Z"
    },
    "id": "qxeL2VoxJDIj",
    "outputId": "8a8366e4-eb9a-47f3-b3ef-96e2f94e6224",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///kaggle/working/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: datasets<=3.4.1,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.3.1)\n",
      "Requirement already satisfied: accelerate<=1.5.2,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.2.1)\n",
      "Requirement already satisfied: peft<=0.15.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.21.0)\n",
      "Collecting gradio<=5.21.0,>=4.38.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.13.1)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.8.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.20.3)\n",
      "Collecting uvicorn (from llamafactory==0.9.3.dev0)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.11.0a2)\n",
      "Collecting fastapi (from llamafactory==0.9.3.dev0)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
      "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.7.5)\n",
      "Collecting fire (from llamafactory==0.9.3.dev0)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
      "Collecting pydantic (from llamafactory==0.9.3.dev0)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting av (from llamafactory==0.9.3.dev0)\n",
      "  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.10.2.post1)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.2.4)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.42.1)\n",
      "Collecting rouge-chinese (from llamafactory==0.9.3.dev0)\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.5.1+cu121)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.29.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.12)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (22.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.7.1)\n",
      "Collecting ffmpy (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.7.2 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.4)\n",
      "Collecting markupsafe~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.12)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.0.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
      "Collecting python-multipart>=0.0.18 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.12.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.3.dev0) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic->llamafactory==0.9.3.dev0)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2->llamafactory==0.9.3.dev0) (2024.11.6)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.3.dev0) (2.5.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk->llamafactory==0.9.3.dev0) (1.17.0)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.18.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0->llamafactory==0.9.3.dev0) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Downloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Downloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Building wheels for collected packages: llamafactory, fire\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26105 sha256=284f65f37a19812d713db9fe124b7ee6a02c2100e7b878aef739da99b3899a48\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-coiibzwz/wheels/21/5a/a2/9a8fea19e68e32089e22401d08554f51119f2464cad3a126ec\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=0d4d563651de588db7dfdfa77bff3abe5ffc7479ba6767118e82ef875cbe4dbc\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
      "Successfully built llamafactory fire\n",
      "Installing collected packages: uvicorn, tomlkit, shtab, semantic-version, ruff, rouge-chinese, python-multipart, pydantic-core, markupsafe, groovy, fire, ffmpy, av, anyio, starlette, pydantic, tyro, sse-starlette, safehttpx, gradio-client, fastapi, transformers, trl, gradio, llamafactory\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.29.0\n",
      "    Uninstalling pydantic_core-2.29.0:\n",
      "      Successfully uninstalled pydantic_core-2.29.0\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.7.1\n",
      "    Uninstalling anyio-3.7.1:\n",
      "      Successfully uninstalled anyio-3.7.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.0a2\n",
      "    Uninstalling pydantic-2.11.0a2:\n",
      "      Successfully uninstalled pydantic-2.11.0a2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.0\n",
      "    Uninstalling transformers-4.47.0:\n",
      "      Successfully uninstalled transformers-4.47.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anyio-4.9.0 av-14.2.0 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 llamafactory-0.9.3.dev0 markupsafe-2.1.5 pydantic-2.10.6 pydantic-core-2.27.2 python-multipart-0.0.20 rouge-chinese-1.0.3 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.2.1 starlette-0.46.1 tomlkit-0.13.2 transformers-4.50.0 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.0\n",
      "Collecting deepspeed\n",
      "  Downloading deepspeed-0.16.5.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed) (0.8.0)\n",
      "Collecting hjson (from deepspeed)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.1.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.10.6)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.67.1)\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from deepspeed) (12.570.86)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.17.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deepspeed) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deepspeed) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->deepspeed) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->deepspeed) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->deepspeed) (2024.2.0)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for deepspeed: filename=deepspeed-0.16.5-py3-none-any.whl size=1580586 sha256=74b8a8f4b4f3c4b6f2371739e956d25f4128c256cea83842d458a08af9978a28\n",
      "  Stored in directory: /root/.cache/pip/wheels/cb/fa/e7/98efc76db11fac734a4fae8c19dd08cc24257107e132e674f6\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: hjson, deepspeed\n",
      "Successfully installed deepspeed-0.16.5 hjson-3.1.0\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1+cu121)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187797312 sha256=b267f80a08e516292cdd748056a2178a45b8abedf7fca123292eb17c21c8c87c\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: flash-attn\n",
      "Successfully installed flash-attn-2.7.4.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch,metrics]\"\n",
    "!pip install deepspeed\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:50:33.319755Z",
     "iopub.status.busy": "2025-04-01T12:50:33.319377Z",
     "iopub.status.idle": "2025-04-01T12:50:35.325140Z",
     "shell.execute_reply": "2025-04-01T12:50:35.324382Z",
     "shell.execute_reply.started": "2025-04-01T12:50:33.319726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Value\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load videos mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:50:36.654361Z",
     "iopub.status.busy": "2025-04-01T12:50:36.653861Z",
     "iopub.status.idle": "2025-04-01T12:50:36.678563Z",
     "shell.execute_reply": "2025-04-01T12:50:36.677958Z",
     "shell.execute_reply.started": "2025-04-01T12:50:36.654336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/d/seanjeanmoey/next-qa-dataset/map_vid_vidorID.json\") as file:\n",
    "    video_dir_map = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:50:38.406275Z",
     "iopub.status.busy": "2025-04-01T12:50:38.405997Z",
     "iopub.status.idle": "2025-04-01T12:50:38.410375Z",
     "shell.execute_reply": "2025-04-01T12:50:38.409708Z",
     "shell.execute_reply.started": "2025-04-01T12:50:38.406254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": f\"<video>{sample['question']}\",\n",
    "                \"role\": \"user\"\n",
    "            },\n",
    "            {\n",
    "                \"content\": f\"{sample['answer']}\",\n",
    "                \"role\": \"assistant\"\n",
    "            }\n",
    "        ],\n",
    "        \"videos\": [\n",
    "            f\"/kaggle/input/d/seanjeanmoey/next-qa-dataset/NExTVideo/NExTVideo/{video_dir_map[sample['video']]}.mp4\"\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format MCQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:50:40.119505Z",
     "iopub.status.busy": "2025-04-01T12:50:40.119221Z",
     "iopub.status.idle": "2025-04-01T12:50:40.124590Z",
     "shell.execute_reply": "2025-04-01T12:50:40.123802Z",
     "shell.execute_reply.started": "2025-04-01T12:50:40.119484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reformat_mcq(sample):\n",
    "    choice_labels = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "    choices = [sample[f\"a{i}\"] for i in range(5)]\n",
    "    formatted_choices = \"\\n\".join([f\"{choice_labels[i]}. {choice}\" for i, choice in enumerate(choices)])\n",
    "    \n",
    "    return {\n",
    "        \"video\": sample[\"video\"],\n",
    "        \"frame_count\": sample[\"frame_count\"],\n",
    "        \"width\": sample[\"width\"],\n",
    "        \"height\": sample[\"height\"],\n",
    "        \"question\": f\"{sample['question']}\\n{formatted_choices}\\nSelect one best answer to the above multiple-choice question based on the video. Respond with only the letter (A, B, C, D or E) of the correct option.\",\n",
    "        \"answer\": choice_labels[sample[\"answer\"]],\n",
    "        \"qid\": sample[\"qid\"],\n",
    "        \"type\": sample[\"type\"],\n",
    "        \"additional_ref_answer\": None\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:50:43.012442Z",
     "iopub.status.busy": "2025-04-01T12:50:43.012149Z",
     "iopub.status.idle": "2025-04-01T12:50:54.851269Z",
     "shell.execute_reply": "2025-04-01T12:50:54.850556Z",
     "shell.execute_reply.started": "2025-04-01T12:50:43.012421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3542d6faf0894b0b8cbb098431bc19b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597ea0c239d94c50a6c2033ab7f37f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9662c6ad98b046cfa7f108b22143ef83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8564 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a1cb5ec95447898ecf7352c5210469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8564 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285b36efaef54291941d76753d550a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0992e9ce6f49809576ca79c9a66151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2864b3a4ed64ea8b7f01e04be8dcbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/289k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42394ba22f4e432cb048f224fac77389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/572k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9392372a9d042b3ade955cfcc7a1979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/37523 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a33a4f67f74b358ae60767acb56be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b0c95c33c446e3b5a5e28689364855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/9178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_id = 'lmms-lab/NExTQA'\n",
    "\n",
    "mcq_dataset = load_dataset(dataset_id, 'MC')['test'].map(reformat_mcq, remove_columns=['a0', 'a1', 'a2', 'a3', 'a4'])\n",
    "new_features = mcq_dataset.features.copy()\n",
    "new_features[\"video\"] = Value(\"string\")\n",
    "new_features[\"frame_count\"] = Value(\"int32\")\n",
    "new_features[\"width\"] = Value(\"int32\")\n",
    "new_features[\"height\"] = Value(\"int32\")\n",
    "new_features[\"qid\"] = Value(\"int32\")\n",
    "mcq_dataset = mcq_dataset.cast(new_features)\n",
    "train_test_split = mcq_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "val_test_split = train_test_split['test'].train_test_split(test_size=2/3, seed=42)\n",
    "mcq_train_dataset = train_test_split['train']\n",
    "mcq_eval_dataset = val_test_split['train']\n",
    "mcq_test_dataset = val_test_split['test']\n",
    "\n",
    "oe_train_dataset, oe_eval_dataset, oe_test_dataset = load_dataset(dataset_id, 'OE', split=['train', 'validation', 'test'])\n",
    "\n",
    "train_dataset = concatenate_datasets([mcq_train_dataset, oe_train_dataset])\n",
    "eval_dataset = concatenate_datasets([mcq_eval_dataset, oe_eval_dataset])\n",
    "test_dataset = concatenate_datasets([mcq_test_dataset, oe_test_dataset])\n",
    "\n",
    "train_dataset = [format_data(sample) for sample in train_dataset]\n",
    "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
    "test_dataset = [format_data(sample) for sample in test_dataset]\n",
    "\n",
    "dataset = train_dataset + eval_dataset + test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset info json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:50:56.190284Z",
     "iopub.status.busy": "2025-04-01T12:50:56.189954Z",
     "iopub.status.idle": "2025-04-01T12:50:56.195658Z",
     "shell.execute_reply": "2025-04-01T12:50:56.194618Z",
     "shell.execute_reply.started": "2025-04-01T12:50:56.190256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = { \n",
    "    \"nextqa\": {\n",
    "        \"file_name\": \"nextqa.json\",\n",
    "        \"formatting\": \"sharegpt\",\n",
    "        \"columns\": {\n",
    "            \"messages\": \"messages\",\n",
    "            \"videos\": \"videos\"\n",
    "        },\n",
    "        \"tags\": {\n",
    "            \"role_tag\": \"role\",\n",
    "            \"content_tag\": \"content\",\n",
    "            \"user_tag\": \"user\",\n",
    "            \"assistant_tag\": \"assistant\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "with open(\"data/dataset_info.json\", \"w\", encoding=\"utf-8\") as f: \n",
    "    json.dump(args, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:50:58.642944Z",
     "iopub.status.busy": "2025-04-01T12:50:58.642550Z",
     "iopub.status.idle": "2025-04-01T12:50:59.770286Z",
     "shell.execute_reply": "2025-04-01T12:50:59.769329Z",
     "shell.execute_reply.started": "2025-04-01T12:50:58.642915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/nextqa.json\", \"w\", encoding=\"utf-8\") as f: \n",
    "    json.dump(dataset, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:51:01.090835Z",
     "iopub.status.busy": "2025-04-01T12:51:01.090503Z",
     "iopub.status.idle": "2025-04-01T12:51:01.094461Z",
     "shell.execute_reply": "2025-04-01T12:51:01.093650Z",
     "shell.execute_reply.started": "2025-04-01T12:51:01.090809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:51:07.031504Z",
     "iopub.status.busy": "2025-04-01T12:51:07.031221Z",
     "iopub.status.idle": "2025-04-01T12:51:08.815394Z",
     "shell.execute_reply": "2025-04-01T12:51:08.814709Z",
     "shell.execute_reply.started": "2025-04-01T12:51:07.031482Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=UserSecretsClient().get_secret(\"WANDB_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create fine-tuning script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:51:10.760027Z",
     "iopub.status.busy": "2025-04-01T12:51:10.759730Z",
     "iopub.status.idle": "2025-04-01T12:51:10.765971Z",
     "shell.execute_reply": "2025-04-01T12:51:10.765002Z",
     "shell.execute_reply.started": "2025-04-01T12:51:10.760006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"model_name_or_path\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    # \"image_max_pixels\": 262144,\n",
    "    # \"video_max_pixels\": 16384,\n",
    "    # \"trust_remote_code\": True,\n",
    "    \"stage\": \"sft\",\n",
    "    \"do_train\": True,\n",
    "    \"finetuning_type\": \"lora\",\n",
    "    \"deepspeed\": \"examples/deepspeed/ds_z3_offload_config.json\",\n",
    "    \"use_fast_tokenizer\": True,\n",
    "    \"flash_attn\": True,\n",
    "    # \"lora_rank\": 8,\n",
    "    \"lora_target\": \"all\",\n",
    "    \"dataset\": \"nextqa\",\n",
    "    \"template\": \"qwen2_vl\",\n",
    "    # \"cutoff_len\": 2048,\n",
    "    # \"max_samples\": 1000,\n",
    "    \"overwrite_cache\": True,\n",
    "    \"preprocessing_num_workers\": 16,\n",
    "    # \"dataloader_num_workers\": 4,\n",
    "    \"output_dir\": \"/kaggle/working/finetuned\",\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 500,\n",
    "    \"plot_loss\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    # \"save_only_model\": False,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 5.0e-5,\n",
    "    \"num_train_epochs\": 3.0,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 0.1,\n",
    "    # \"warmup_ratio\": 0.1,\n",
    "    \"bf16\": True,\n",
    "    \"ddp_timeout\": 180000000,\n",
    "    # \"resume_from_checkpoint\": None,\n",
    "    # \"val_size\": 1,\n",
    "    # \"per_device_eval_batch_size\": 1,\n",
    "    # \"eval_strategy\": \"steps\",\n",
    "    # \"eval_steps\": 500,\n",
    "}\n",
    "with open(\"train.json\", \"w\", encoding=\"utf-8\") as f: \n",
    "    json.dump(args, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:51:13.296164Z",
     "iopub.status.busy": "2025-04-01T12:51:13.295855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-01 12:51:23.726943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-01 12:51:24.084989: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-01 12:51:24.189348: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[2025-04-01 12:51:41,037] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[INFO|2025-04-01 12:51:48] llamafactory.cli:143 >> Initializing 2 distributed tasks at: 127.0.0.1:57881\n",
      "W0401 12:51:49.727000 339 torch/distributed/run.py:793] \n",
      "W0401 12:51:49.727000 339 torch/distributed/run.py:793] *****************************************\n",
      "W0401 12:51:49.727000 339 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0401 12:51:49.727000 339 torch/distributed/run.py:793] *****************************************\n",
      "2025-04-01 12:51:55.521656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-01 12:51:55.534285: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-01 12:51:55.543990: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-01 12:51:55.550441: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 12:51:55.556161: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-01 12:51:55.562818: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[2025-04-01 12:51:58,920] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-01 12:51:58,926] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-01 12:52:01,336] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-04-01 12:52:01,347] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-04-01 12:52:01,348] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[WARNING|2025-04-01 12:52:01] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-04-01 12:52:01] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-04-01 12:52:01] llamafactory.hparams.parser:383 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "tokenizer_config.json: 100%|███████████████| 7.23k/7.23k [00:00<00:00, 29.0MB/s]\n",
      "vocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 15.0MB/s]\n",
      "merges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 18.7MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 26.8MB/s]\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:02,534 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:02,534 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:02,534 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:02,534 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:02,534 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:02,534 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:02,534 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-01 12:52:02,940 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "preprocessor_config.json: 100%|████████████████| 350/350 [00:00<00:00, 2.81MB/s]\n",
      "[INFO|image_processing_base.py:381] 2025-04-01 12:52:03,106 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:381] 2025-04-01 12:52:03,143 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n",
      "[WARNING|logging.py:329] 2025-04-01 12:52:03,143 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[INFO|image_processing_base.py:434] 2025-04-01 12:52:03,144 >> Image processor Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:03,184 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:03,184 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:03,184 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:03,185 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:03,185 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:03,185 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-01 12:52:03,185 >> loading file chat_template.jinja from cache at None\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-01 12:52:03,541 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "chat_template.json: 100%|██████████████████| 1.05k/1.05k [00:00<00:00, 6.26MB/s]\n",
      "[INFO|processing_utils.py:876] 2025-04-01 12:52:04,303 >> Processor Qwen2_5_VLProcessor:\n",
      "- image_processor: Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n",
      ")\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\"\n",
      "}\n",
      "\n",
      "[INFO|2025-04-01 12:52:04] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
      "[INFO|2025-04-01 12:52:04] llamafactory.data.loader:143 >> Loading dataset nextqa.json...\n",
      "Generating train split: 60608 examples [00:00, 98386.00 examples/s] \n",
      "Converting format of dataset:   0%| | 236/60608 [00:00<02:45, 363.82 examples/s][rank1]:[W401 12:52:05.331134443 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Converting format of dataset: 100%|█| 60608/60608 [00:27<00:00, 2202.28 examples\n",
      "[rank0]:[W401 12:52:32.083884957 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Running tokenizer on dataset:   0%|            | 0/60608 [00:00<?, ? examples/s]"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train train.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Fine-tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-01T08:52:36.170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create merging script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-01T08:52:36.171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"model_name_or_path\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    \"adapter_name_or_path\": \"/kaggle/working/finetuned\",\n",
    "    \"template\": \"qwen2_vl\",\n",
    "    \"finetuning_type\": \"lora\",\n",
    "    # \"trust_remote_code\": True,\n",
    "    \"export_dir\": \"/kaggle/working/merged\",\n",
    "    \"export_size\": 5,\n",
    "    \"export_device\": \"cpu\",\n",
    "    # \"export_legacy_format\": False,\n",
    "}\n",
    "with open(\"merge.json\", \"w\", encoding=\"utf-8\") as f: \n",
    "    json.dump(args, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-01T08:52:36.171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!llamafactory-cli export merge.json"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6989634,
     "sourceId": 11200209,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18ab9fe8abd84251a729fefe3bfa0ad9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23d09e6258784367bee4067b800fceb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48d7994725b24a28ac9c79418c5111a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7417c346afc24a98832d481d58167b69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae6c27aa30f34ddeba9abd06bf96cafb",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93bdf8798f5f42718bb89b101e54c682",
      "value": 2
     }
    },
    "7d5805c372ee40cdb860c69d0d87c87b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8e66c28e3e54a3ea111ef70b229d8a9",
      "placeholder": "​",
      "style": "IPY_MODEL_23d09e6258784367bee4067b800fceb9",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "93bdf8798f5f42718bb89b101e54c682": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae6c27aa30f34ddeba9abd06bf96cafb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b11f7e2e0acc40128f1ffc5f36b5b029": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7d5805c372ee40cdb860c69d0d87c87b",
       "IPY_MODEL_7417c346afc24a98832d481d58167b69",
       "IPY_MODEL_f73f2c3f4e1347dcb0ff085a27d1927a"
      ],
      "layout": "IPY_MODEL_48d7994725b24a28ac9c79418c5111a6"
     }
    },
    "b5dbaea6b0ce4330921b1486f1c4f7d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8e66c28e3e54a3ea111ef70b229d8a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f73f2c3f4e1347dcb0ff085a27d1927a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18ab9fe8abd84251a729fefe3bfa0ad9",
      "placeholder": "​",
      "style": "IPY_MODEL_b5dbaea6b0ce4330921b1486f1c4f7d8",
      "value": " 2/2 [00:34&lt;00:00, 17.13s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
